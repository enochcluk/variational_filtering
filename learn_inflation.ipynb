{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, jit, lax\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "from jax.numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from jax_models import Lorenz96\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix\n",
    "from jax_filters import ensrf_steps\n",
    "#from jax_vi import KL_gaussian, log_likelihood\n",
    "\n",
    "\n",
    "# Parameters\n",
    "F = 8.0\n",
    "dt = 0.01\n",
    "num_steps = 30  # Number of time steps\n",
    "n_timesteps = num_steps\n",
    "J0 = 0\n",
    "n = 40   # Number of state variables\n",
    "Q = 0.1 * np.eye(n)  # Process noise covariance\n",
    "R_matrix = make_spd_matrix(n)  # Generating a symmetric positive definite matrix for R\n",
    "R = np.array(R_matrix)  # Observation noise covariance\n",
    "inv_R = inv(R)\n",
    "H = np.eye(n)  # Observation matrix\n",
    "\n",
    "\n",
    "N = 10\n",
    "n_ensemble = 20\n",
    "observation_interval = 1\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "\n",
    "l96_model = Lorenz96(dt = 0.01, F = 8)\n",
    "state_transition_function = l96_model.step\n",
    "# Generate true states and observations using the Lorenz '96 model\n",
    "key = random.PRNGKey(0)\n",
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_model.step, observation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lax import scan\n",
    "\n",
    "@jit\n",
    "def log_likelihood(v, y, H, inv_R, R, J, J0):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of observations given state estimates.\n",
    "    \"\"\"\n",
    "    def log_likelihood_j(_, v_y):\n",
    "        v_j, y_j = v_y\n",
    "        error = y_j - H @ v_j\n",
    "        ll = error.T @ inv_R @ error\n",
    "        return _, ll\n",
    "    _, lls = lax.scan(log_likelihood_j, None, (v, y))\n",
    "    sum_ll = sum(lls)\n",
    "    return -0.5 * sum_ll - 0.5 * (J - J0) * np.log(2 * np.pi) - 0.5 * (J - J0) * np.log(det(R))\n",
    "\n",
    "\n",
    "@jit\n",
    "def KL_gaussian(m1, C1, m2, C2):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler divergence between two Gaussian distributions.\n",
    "    m1, C1: Mean and covariance of the first Gaussian distribution.\n",
    "    m2, C2: Mean and covariance of the second Gaussian distribution.\n",
    "    n: number of state variables\n",
    "    \"\"\"\n",
    "    C2_inv = inv(C2)\n",
    "    log_det_ratio = (np.log(np.linalg.eigvals(C2)).sum() - np.log(np.linalg.eigvals(C1)).sum()).real # log(det(C2) / det(C1)), works better with limited precision because the determinant is practically 0\n",
    "    return 0.5 * (log_det_ratio - n + np.trace(C2_inv @ C1) + ((m2 - m1).T @ C2_inv @ (m2 - m1)))\n",
    "\n",
    "\n",
    "@jit\n",
    "def KL_sum(m, C, Q, key):\n",
    "    \"\"\"\n",
    "    Computes the sum of KL divergences between the predicted and updated state distributions.\n",
    "    \"\"\"\n",
    "    def KL_j(_, m_C_y):\n",
    "        m_prev, m_curr, C_prev, C_curr, key = m_C_y\n",
    "        key, *subkeys_inner = random.split(key, num=N)\n",
    "        def inner_map(subkey):\n",
    "            perturbed_state = m_prev + random.multivariate_normal(subkey, np.zeros(n), C_prev)\n",
    "            m_pred = state_transition_function(perturbed_state)\n",
    "            return KL_gaussian(m_curr, C_curr, m_pred, Q) #not sure if use of Q here is correct\n",
    "        mean_kl = np.mean(lax.map(inner_map, np.array(subkeys_inner)), axis=0)\n",
    "        return _, mean_kl\n",
    "\n",
    "    _, mean_kls = scan(KL_j, None, (m[:-1, :], m[1:, :], C[:-1, :, :], C[1:, :, :], np.array(random.split(key, num=m.shape[0]-1))))\n",
    "    kl_sum = sum(mean_kls)\n",
    "    return kl_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def ensrf_step(ensemble, y, H, Q, R, localization_matrix, inflation):\n",
    "    n_ensemble = ensemble.shape[1]\n",
    "    x_m = np.mean(ensemble, axis=1)\n",
    "    A = ensemble - x_m.reshape((-1, 1))\n",
    "    Pf = inflation * A @ A.T / (n_ensemble - 1)\n",
    "    P = Pf * localization_matrix + Q  # Element-wise multiplication for localization\n",
    "    K = P @ H.T @ np.linalg.inv(H @ P @ H.T + R)\n",
    "    x_m += K @ (y - H @ x_m)\n",
    "    M = np.eye(x_m.shape[0]) + P @ H.T @ np.linalg.inv(R) @ H\n",
    "    # U, s, Vh = svd(M)\n",
    "    # s_inv_sqrt = np.diag(s**-0.5)\n",
    "    # M_inv_sqrt = U @ s_inv_sqrt @ Vh\n",
    "    eigenvalues, eigenvectors = eigh(M)\n",
    "    inv_sqrt_eigenvalues = 1 / np.sqrt(eigenvalues)\n",
    "    Lambda_inv_sqrt = np.diag(inv_sqrt_eigenvalues)\n",
    "    M_inv_sqrt = eigenvectors @ Lambda_inv_sqrt @ eigenvectors.T\n",
    "    updated_ensemble = x_m.reshape((-1, 1)) + M_inv_sqrt @ A\n",
    "    return updated_ensemble, P  # Now also returning P\n",
    "\n",
    "@jit\n",
    "def ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation):\n",
    "    def inner(carry, t):\n",
    "        ensemble, covariances = carry\n",
    "        obs = observations[t, :]\n",
    "        ensemble_updated, P_updated = lax.cond(\n",
    "            t % observation_interval == 0,\n",
    "            lambda _: ensrf_step(ensemble, obs, H, Q, R, localization_matrix, inflation),\n",
    "            lambda _: (ensemble, np.zeros_like(Q)),  # Return zero covariance for non-observation steps\n",
    "            None)\n",
    "        covariances = covariances.at[t].set(P_updated)\n",
    "        return (ensemble_updated, covariances), ensemble_updated\n",
    "\n",
    "    covariances_init = np.zeros((n_timesteps, *Q.shape))\n",
    "    _, states = lax.scan(inner, (ensemble_init, covariances_init), np.arange(n_timesteps))\n",
    "\n",
    "    return states, covariances_init\n",
    "\n",
    "\n",
    "@jit\n",
    "def var_cost(inflation, ensemble_init, observations, H, Q, R, localization_matrix, key, J, J0):\n",
    "   \n",
    "    states, covariances = ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation)\n",
    "    ensemble_mean = np.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(ensemble_mean, covariances, Q, key)\n",
    "    \n",
    "\n",
    "    # Calculate log-likelihood values for a batch of perturbed states\n",
    "    log_likelihood_vals = lax.map(\n",
    "        lambda subkey: log_likelihood(\n",
    "            random.multivariate_normal(subkey, ensemble_mean, covariances),\n",
    "            observations, H, np.linalg.inv(R), R, J, J0),\n",
    "        np.array(subkeys))\n",
    "\n",
    "    cost = kl_sum - np.mean(log_likelihood_vals)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd53c9075f541fd891c1b66c4b76481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 40, 20)\n",
      "RMSE: 0.5502030849456787\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n",
      "(30, 40, 20)\n",
      "RMSE: nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from jax import grad\n",
    "from tqdm.notebook import tqdm\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "\n",
    "\n",
    "# Modification: Use grad to compute the gradient with respect to the inflation parameter\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "inflation_opt = 1.0  # Example starting value for inflation\n",
    "alpha = 1e-6  # Learning rate\n",
    "key = random.PRNGKey(0)  # Random key\n",
    "N = 10  # Number of MC samples\n",
    "m0 = initial_state\n",
    "C0 = Q  # Initial covariance, assuming Q is your process noise covariance\n",
    "localization_matrix = generate_gc_localization_matrix(n, 5)\n",
    "ensemble_init = random.multivariate_normal(key, initial_state, Q, (n_ensemble,)).T\n",
    "\n",
    "rmses = []\n",
    "norms = []\n",
    "\n",
    "for i in tqdm(range(20)):\n",
    "    key, subkey = random.split(key)\n",
    "    \n",
    "    states, _ = ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation_opt)\n",
    "    print(states.shape)\n",
    "    ensemble_mean = np.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "    rmse = np.sqrt(np.mean((ensemble_mean - true_states)**2))\n",
    "    rmses.append(rmse)\n",
    "    #clear_output(wait=True)\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    \n",
    "    # Gradient descent step for inflation parameter\n",
    "    grad_inflation = var_cost_grad(inflation_opt, ensemble_init, observations, H, Q, R, localization_matrix, subkey, num_steps, J0)\n",
    "    inflation_opt -= alpha * grad_inflation  # Update inflation parameter\n",
    "    print(inflation_opt)\n",
    "    \n",
    "    norms.append(np.linalg.norm(inflation_opt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
