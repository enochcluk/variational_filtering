{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, lax, jacrev\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "from jax.numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from jax_models import Lorenz96\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix\n",
    "from jax_filters import ensrf_steps, kalman_filter_process\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "#from jax_vi import KL_gaussian, log_likelihood\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "F = 8.0\n",
    "dt = 0.05\n",
    "num_steps = 250  # Number of time steps\n",
    "\n",
    "J0 = 0\n",
    "n = 40   # Number of state variables\n",
    "Q = 0.2 * jnp.eye(n)  # Process noise covariance\n",
    "R_matrix = 0.5 * jnp.eye(n)\n",
    "R = jnp.array(R_matrix)  # Observation noise covariance\n",
    "inv_R = inv(R)\n",
    "H = jnp.eye(n)  # Observation matrix\n",
    "\n",
    "\n",
    "N = 10\n",
    "n_ensemble = 10\n",
    "observation_interval = 1\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "m0 = initial_state\n",
    "C0 = Q\n",
    "\n",
    "l96_model = Lorenz96(dt = dt, F = F)\n",
    "state_transition_function = l96_model.step\n",
    "l96_step = Partial(state_transition_function)\n",
    "jacobian_function = jacrev(l96_step, argnums=0)\n",
    "jac_func = Partial(jacobian_function)\n",
    "# Generate true states and observations using the Lorenz '96 model\n",
    "key = random.PRNGKey(0)\n",
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_step, observation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "radius = 10\n",
    "localization_matrix = generate_gc_localization_matrix(n, radius)\n",
    "ensemble_init = random.multivariate_normal(key, initial_state, Q, (n_ensemble,)).T\n",
    "inflation = 1\n",
    "e, c = ensrf_steps(l96_step, n_ensemble, ensemble_init, num_steps, observations, observation_interval, H, Q, R, localization_matrix, inflation, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.pcolormesh(c[-1])\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax_vi import KL_gaussian, log_likelihood, KL_sum\n",
    "\n",
    "def var_cost(inflation, ensemble_init, observations, H, Q, R, localization_matrix, key, num_steps, J0):\n",
    "   \n",
    "    states, covariances = ensrf_steps(l96_step, n_ensemble, ensemble_init, num_steps, observations, observation_interval, H, Q, R, localization_matrix, inflation, key)\n",
    "    ensemble_mean = jnp.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(ensemble_mean, covariances, n, l96_step, Q, key, N)\n",
    "\n",
    "    def inner_map(subkey):\n",
    "        return log_likelihood(random.multivariate_normal(subkey, ensemble_mean, covariances), observations, H, R, num_steps, J0)  # Sometimes the covariances are negative definite. Fix\n",
    "    cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(107884.19, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_cost(inflation, ensemble_init, observations, H, Q, R, localization_matrix, key, num_steps, J0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m, C, K = kalman_filter_process(l96_step, jac_func, m0, C0, observations, H, Q, R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb4a6ad13f54384ac5160476a2246a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from jax import grad\n",
    "from tqdm.notebook import tqdm\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import properscoring\n",
    "\n",
    "\n",
    "# Modification: Use grad to compute the gradient with respect to the inflation parameter\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "inflation_opt = 1.3 # Example starting value for inflation\n",
    "alpha = 1e-6  # Learning rate\n",
    "key = random.PRNGKey(0)  # Random key\n",
    "N = 10  # Number of MC samples\n",
    "m0 = initial_state\n",
    "C0 = Q  # Initial covariance, assuming Q is your process noise covariance\n",
    "localization_matrix = generate_gc_localization_matrix(n, radius)# jnp.ones((n, n)) # \n",
    "ensemble_init = random.multivariate_normal(key, initial_state, Q, (n_ensemble,)).T\n",
    "\n",
    "crpss = []\n",
    "rmses=[]\n",
    "inflations = []\n",
    "errs = []\n",
    "true_div = []\n",
    "\n",
    "\n",
    "# from jax import config\n",
    "# config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "for i in tqdm(range(50)):\n",
    "    key, subkey = random.split(key)\n",
    "        \n",
    "    # Gradient descent step for inflation parameter\n",
    "    grad_inflation = var_cost_grad(inflation_opt, ensemble_init, observations, H, Q, R, localization_matrix, subkey, num_steps, J0)\n",
    "    inflation_opt -= alpha * grad_inflation  # Update inflation parameter\n",
    "    \n",
    "    inflations.append(inflation_opt)\n",
    "\n",
    "    states, covariances = ensrf_steps(l96_step, n_ensemble, ensemble_init, num_steps, observations, observation_interval, H, Q, R, localization_matrix, inflation_opt, key)\n",
    "    ensemble_mean = jnp.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "    rmse = jnp.sqrt(jnp.mean((ensemble_mean - true_states)**2))\n",
    "    rmses.append(rmse)\n",
    "    crps = properscoring.crps_ensemble(true_states, states).mean(axis=1).mean()\n",
    "    crpss.append(crps)\n",
    "    total_kl_divergence = 0\n",
    "    for t in range(num_steps):  \n",
    "        kl_div_t = KL_gaussian(n, m[t], C[t], ensemble_mean[t], covariances[t])\n",
    "        total_kl_divergence += kl_div_t\n",
    "    \n",
    "    true_div.append(total_kl_divergence / 250)\n",
    "    #clear_output(wait=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 40, 40)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plotting the RMSE and CRPS on the primary y-axis\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Divergence', color=color)\n",
    "ln1 = ax1.plot(true_div, label='Divergence from True Solution', color='tab:orange', linestyle='-')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a twin y-axis for the inflation parameter\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Inflation', color=color)\n",
    "ln3 = ax2.plot(inflations, label='Inflation Parameter', color=color, linestyle=':')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Combine legends from both axes\n",
    "lns = ln1 + ln3\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc='upper right')\n",
    "\n",
    "plt.title('Inflation Parameter and Divergence from True Solution over Iterations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
