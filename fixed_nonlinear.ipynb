{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, jacfwd, jacrev\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "from jax.lax import scan\n",
    "from scipy.linalg import solve_discrete_are, norm\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.tree_util import Partial\n",
    "from functools import partial\n",
    "from jax_vi import KL_gaussian, log_likelihood, KL_sum, plot_optimization_results\n",
    "from jax_filters import apply_filtering_fixed_nonlinear, kalman_filter_process, filter_step_nonlinear\n",
    "from jax_models import visualize_observations, Lorenz96, KuramotoSivashinsky, generate_true_states, generate_localization_matrix\n",
    "\n",
    "key = random.PRNGKey(3)\n",
    "\n",
    "# System dimensions\n",
    "n = 40  # System dimension\n",
    "p = 2  # Observation dimension\n",
    "J0 = 0 # burn in period\n",
    "N = 10 # Monte Carlo samples for cost function sampling\n",
    "F = 8.0\n",
    "dt = 0.05\n",
    "num_steps = 1000  # Number of time steps\n",
    "\n",
    "# Model parameters\n",
    "m0 = jnp.ones((n,))\n",
    "C0 = jnp.eye(n) * 1.0   # Initial state covariance matrix (P)\n",
    "Q = jnp.eye(n)* 0.1   # Process noise covariance matrix (Sigma in Julia code). We use diagonal for nonlinear case\n",
    "H = jnp.eye(n)          # Observation noise matrix \n",
    "R = jnp.eye(H.shape[0])  # R is dependent on number of observed states (also written as Gamma)\n",
    "\n",
    "observation_interval = 1\n",
    "\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "l96_model = Lorenz96(dt = dt, F = F)\n",
    "state_transition_function = l96_model.step\n",
    "l96_step = Partial(state_transition_function)\n",
    "\n",
    "# Generate true states and observations using the Lorenz '96 model\n",
    "key = random.PRNGKey(0)\n",
    "jacobian_function = jacrev(l96_step, argnums=0)\n",
    "jac_func = Partial(jacobian_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_step, observation_interval)\n",
    "y = observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(3,10))\n",
    "def var_cost(K, m0, C0, n, H, Q, R, y, key, num_steps, J0):\n",
    "    m_preds, C_preds, m_updates, C_updates = apply_filtering_fixed_nonlinear(m0, C0, y, K, n, l96_step, jac_func, H, Q, R)\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(m_preds, C_preds, m_updates, C_updates, n, l96_step, Q, key)\n",
    "    def inner_map(subkey):\n",
    "        return log_likelihood(random.multivariate_normal(subkey, m_updates, C_updates), y, H, R, num_steps, J0) \n",
    "    cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "    print(cost)\n",
    "    return cost\n",
    "\n",
    "@partial(jit, static_argnums=(3))\n",
    "def var_cost_single_step(K, m0, C0, n, Q, H, R, y_curr, key, J, J0):\n",
    "    (m_update, C_update), _  =  filter_step_nonlinear((m0,C0), y_curr, K, n, l96_step, jac_func, H, Q, R)\n",
    "    log_likelihood_val = log_likelihood(m_update[jnp.newaxis, :], y_curr[jnp.newaxis, :], H, R, J=1, J0=J0)\n",
    "    # Calculate the KL divergence between the predicted and updated state distributions\n",
    "    m_pred = state_transition_function(m0)\n",
    "    M = jac_func(m0)\n",
    "    C_pred = M @ C0 @ M.T + Q\n",
    "    kl_divergence = KL_gaussian(n, m_update, C_update, m_pred, C_pred)\n",
    "    # Combine the KL divergence and the negative log-likelihood to form the cost\n",
    "    cost = kl_divergence - log_likelihood_val\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_m, base_C, base_K = kalman_filter_process(l96_step, jac_func, m0, C0, observations, H, Q, R)\n",
    "K_steady = jnp.mean(base_K[-50:, :, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d7fbc81b04495e81fcc0323317c9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=3/0)>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     36\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorms\u001b[39m\u001b[38;5;124m'\u001b[39m: norms,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_errors\u001b[39m\u001b[38;5;124m'\u001b[39m: prediction_errors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK_opt\u001b[39m\u001b[38;5;124m'\u001b[39m: K_opt\n\u001b[1;32m     43\u001b[0m }\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfixed_nonlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 45\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(data, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "# Initial guess for K and optimization parameters\n",
    "K_opt = jnp.eye(n) * 0.4\n",
    "\n",
    "alpha = 1e-5\n",
    "\n",
    "online = False\n",
    "prediction_errors = [] \n",
    "norms = []\n",
    "true_div = []\n",
    "last_200_errors = []\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "    key, _ = random.split(key)\n",
    "    # Update the gradient and Kalman gain\n",
    "    grad_K = var_cost_grad(K_opt, m0, C0, n, H, Q, R, y, key, num_steps, J0)\n",
    "    K_opt -= alpha * grad_K\n",
    "    \n",
    "    # Apply filtering with the newly optimized K to generate state predictions\n",
    "    _, _, predicted_states, covariances = apply_filtering_fixed_nonlinear(m0, C0, y, K_opt, n, l96_step, jac_func, H, Q, R)\n",
    "    \n",
    "    prediction_error = jnp.mean(jnp.mean((predicted_states - true_states)**2, axis=1))\n",
    "    prediction_errors.append(prediction_error)\n",
    "    last_200_prediction_error = jnp.mean((predicted_states[-200:] - true_states[-200:])**2)\n",
    "    last_200_errors.append(last_200_prediction_error)\n",
    "    norms.append(norm(K_opt - K_steady))\n",
    "    total_kl_divergence = 0\n",
    "    for t in range(num_steps):  \n",
    "        kl_div_t = KL_gaussian(n, predicted_states[t], covariances[t],  base_m[t], base_C[t])\n",
    "        total_kl_divergence += kl_div_t \n",
    "    true_div.append(total_kl_divergence / num_steps)\n",
    "    \n",
    "\n",
    "data = {\n",
    "    'norms': norms,\n",
    "    'prediction_errors': prediction_errors,\n",
    "    'true_div': true_div,\n",
    "    'n_iters': n_iters,\n",
    "    'K_steady': K_steady,\n",
    "    'K_opt': K_opt\n",
    "}\n",
    "with open('fixed_nonlinear', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data = load_data('data.pkl')\n",
    "# norms = data['norms']\n",
    "# prediction_errors = data['prediction_errors']\n",
    "# true_div = data['true_div']\n",
    "# n_iters = data['n_iters']\n",
    "# K_steady = data['K_steady']\n",
    "# K_opt = data['K_opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('fixed_nonlinear', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "online = False\n",
    "norms = data['norms']\n",
    "prediction_errors = data['prediction_errors']\n",
    "true_div = data['true_div']\n",
    "n_iters = data['n_iters']\n",
    "K_steady = data['K_steady']\n",
    "K_opt = data['K_opt']\n",
    "import os\n",
    "from jax_vi import plot_optimization_results, plot_k_matrices\n",
    "\n",
    "subfolder_name = 'nonlinear_results'\n",
    "file_base_name = \"nonlinear_gain\"\n",
    "\n",
    "if online:\n",
    "    n_iters = num_steps\n",
    "    file_base_name += \"_online\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "\n",
    "plot_optimization_results(norms, prediction_errors, true_div, n_iters, file_path, scaling=1.3, max_n_locator=3)\n",
    "\n",
    "file_base_name = \"nonlinear_gain_matrices\"\n",
    "if online:\n",
    "    file_base_name += \"_online\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "plot_k_matrices(K_steady, K_opt, file_path, scaling=1.2, max_n_locator=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_base_name = \"nonlinear_gain_matrices\"\n",
    "if online:\n",
    "    file_base_name += \"_online\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "plot_k_matrices(K_steady, K_opt, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_errors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Offline Descent MSE for last 200 timesteps (convergence)\", jnp.mean(jnp.array(last_200_errors[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
