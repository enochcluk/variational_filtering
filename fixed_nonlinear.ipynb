{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, jacfwd, jacrev\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "\n",
    "from jax.lax import scan\n",
    "import jax\n",
    "from scipy.linalg import solve_discrete_are\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "from jax_vi import KL_gaussian, log_likelihood, KL_sum\n",
    "from jax_filters import apply_filtering_fixed_nonlinear, kalman_filter_process, filter_step\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "key = random.PRNGKey(3)\n",
    "\n",
    "# System dimensions\n",
    "n = 40  # System dimension\n",
    "p = 2  # Observation dimension\n",
    "J0 = 0 # burn in period\n",
    "N = 10 # Monte Carlo samples\n",
    "F = 8.0\n",
    "dt = 0.05\n",
    "num_steps = 1000  # Number of time steps\n",
    "\n",
    "# Model parameters\n",
    "m0 = jnp.ones((n,))\n",
    "C0 = jnp.eye(n) * 1.0   # Initial state covariance matrix (P)\n",
    "q = random.normal(key, (n, n))/5\n",
    "Q = q@q.T + jnp.eye(n)*0.1    # Process noise covariance matrix (Sigma in Julia code)\n",
    "Q = jnp.eye(n)*0.1   #jnp.eye(n) * 5.0    # Process noise covariance matrix (Sigma in Julia code)\n",
    "\n",
    "H = jnp.eye(n)          # Observation matrix\n",
    "# H = jnp.eye(n)[::2] #partial observation\n",
    "\n",
    "R = jnp.eye(H.shape[0])  # R now becomes 20x20 for partial H 20*40\n",
    "inv_R = inv(R)\n",
    "\n",
    "observation_interval = 1\n",
    "\n",
    "# State initialization\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "\n",
    "l96_model = Lorenz96(dt = dt, F = F)\n",
    "state_transition_function = l96_model.step\n",
    "l96_step = Partial(state_transition_function)\n",
    "# Generate true states and observations using the Lorenz '96 model\n",
    "key = random.PRNGKey(0)\n",
    "jacobian_function = jacrev(l96_step, argnums=0)\n",
    "jac_func = Partial(jacobian_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_step, observation_interval)\n",
    "y = observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(3,10))\n",
    "def var_cost(K, m0, C0, n, H, Q, R, y, key, num_steps, J0):\n",
    "    states, covariances = apply_filtering_fixed_nonlinear(m0, C0, y, K, n, l96_step, jac_func, H, Q, R)\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(states, covariances, n, l96_step, Q, key, N)\n",
    "\n",
    "    def inner_map(subkey):\n",
    "        return log_likelihood(random.multivariate_normal(subkey, states, covariances), y, H, R, num_steps, J0) \n",
    "    cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "    print(cost)\n",
    "    return cost\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(3))\n",
    "def var_cost_single_step(K, m0, C0, n, Q, H, R, y_curr, key, J, J0):\n",
    "    (m_update, C_update), _  =  filter_step((m0,C0), y_curr, K, n, l96_step, jac_func, H, Q, R)\n",
    "    \n",
    "    log_likelihood_val = log_likelihood(m_update[jnp.newaxis, :], y_curr[jnp.newaxis, :], H, R, J=1, J0=J0)\n",
    "    # Calculate the KL divergence between the predicted and updated state distributions\n",
    "    m_pred = state_transition_function(m0)\n",
    "    M = jac_func(m0)\n",
    "    C_pred = M @ C0 @ M.T + Q\n",
    "    kl_divergence = KL_gaussian(n, m_update, C_update, m_pred, C_pred)\n",
    "    \n",
    "    # Combine the KL divergence and the negative log-likelihood to form the cost\n",
    "    cost = kl_divergence - log_likelihood_val\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 2.9307029e-01, -1.0042849e-02, -2.6016304e-02, ...,\n",
       "        -1.1984844e-02,  1.9772753e-02,  4.0613603e-02],\n",
       "       [-1.0042855e-02,  2.8541455e-01, -6.8869330e-02, ...,\n",
       "        -1.4934078e-02,  2.1845214e-02,  6.7896418e-02],\n",
       "       [-2.6016304e-02, -6.8869330e-02,  2.7601305e-01, ...,\n",
       "        -2.3392761e-04,  3.4112309e-03, -7.6177530e-03],\n",
       "       ...,\n",
       "       [-1.1984842e-02, -1.4934080e-02, -2.3392645e-04, ...,\n",
       "         2.7485043e-01, -2.1949125e-02, -5.0493028e-02],\n",
       "       [ 1.9772749e-02,  2.1845208e-02,  3.4112323e-03, ...,\n",
       "        -2.1949127e-02,  2.7540705e-01,  3.0512437e-02],\n",
       "       [ 4.0613603e-02,  6.7896418e-02, -7.6177469e-03, ...,\n",
       "        -5.0493028e-02,  3.0512441e-02,  2.5728223e-01]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, C, K = kalman_filter_process(l96_step, jac_func, m0, C0, observations, H, Q, R)\n",
    "K_steady = jnp.mean(K[-10:, :, :], axis=0)\n",
    "K_steady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K_opt for partial observation!\n",
    "\n",
    "if H.shape[0] == 40:\n",
    "    K_opt = jnp.eye(n) * 0.4\n",
    "else:\n",
    "    K_opt = jnp.zeros((40, 20))\n",
    "    for i in range(0, K_opt.shape[1]):\n",
    "        K_opt = K_opt.at[i*2, i].set(1)\n",
    "    #K_opt = K_opt + random.normal(key, K_opt.shape) * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(154061.31, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_cost(K_opt, m0, C0, n, H, Q, R, y, key, num_steps, J0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04babd7549a342f5bfe5444ddcf1f548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.numpy import linalg as jnpl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "# Initial guess for K and optimization parameters\n",
    "\n",
    "alpha = 1e-8\n",
    "\n",
    "live = False\n",
    "prediction_errors = [] \n",
    "norms = []\n",
    "true_div = []\n",
    "\n",
    "n_iters = 500\n",
    "num_steps = n_iters\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "    key, _ = random.split(key)\n",
    "    # Update the gradient and Kalman gain\n",
    "    grad_K = var_cost_grad(K_opt, m0, C0, n, H, Q, R, y, key, num_steps, J0)\n",
    "    K_opt -= alpha * grad_K\n",
    "    \n",
    "    # Apply filtering with the newly optimized K to generate state predictions\n",
    "    predicted_states, covariances = apply_filtering_fixed_nonlinear(m0, C0, y, K_opt, n, l96_step, jac_func, H, Q, R)\n",
    "    \n",
    "    prediction_error = np.mean(np.mean((predicted_states - true_states)**2, axis=1))\n",
    "    prediction_errors.append(prediction_error)\n",
    "    norms.append(jnpl.norm(K_opt - K_steady))\n",
    "    total_kl_divergence = 0\n",
    "    for t in range(num_steps):  \n",
    "        kl_div_t = KL_gaussian(n, predicted_states[t], covariances[t],  m[t], C[t])\n",
    "        total_kl_divergence += kl_div_t\n",
    "    \n",
    "    true_div.append(total_kl_divergence / num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from jax.numpy import linalg as jnpl\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# true_div = []\n",
    "# prediction_errors = [] \n",
    "# norms = []\n",
    "# Ks = []\n",
    "# live = True\n",
    "\n",
    "# # Define the gradient of the cost function\n",
    "# var_cost_single_grad = grad(var_cost_single_step, argnums = 0)\n",
    "\n",
    "# # Initial guess for K and optimization parameters\n",
    "# alpha = 1e-5\n",
    "\n",
    "# num_steps = 1000\n",
    "# for i in tqdm(range(num_steps)):\n",
    "#     key, _ = random.split(key)\n",
    "#     y_curr = observations[i] \n",
    "#     # Update the gradient and Kalman gain\n",
    "#     for j in range(100):\n",
    "#         grad_K = var_cost_single_grad(K_opt, m0, C0, n, Q, H, R, y_curr, key, num_steps, J0)\n",
    "#         K_opt -= alpha * grad_K\n",
    "#     Ks.append(K_opt)\n",
    "#     norms.append(jnp.linalg.norm(K_opt - K_steady)) \n",
    "#     (m_update, C_update), _ = filter_step((m0,C0), y_curr, K_opt, n, l96_step, jac_func, H, Q, R)\n",
    "#     prediction_error = np.square(m_update - true_states[i]).mean()  # Assuming true_states[i] is available\n",
    "#     prediction_errors.append(prediction_error)\n",
    "#     true_div.append(KL_gaussian(n, m_update, C_update, m[i], C[i]))\n",
    "#     # Prepare for the next step\n",
    "#     m0, C0 = m_update, C_update\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "# color = 'tab:red'\n",
    "# ax1.set_xlabel('Iteration')\n",
    "# ax1.set_ylabel('$\\|K_\\mathrm{opt} - K_\\mathrm{steady}\\|_F$', color=color)\n",
    "# ax1.plot(norms, label='Gain error', color=color)\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# # Instantiate a second y-axis that shares the same x-axis\n",
    "# ax2 = ax1.twinx()\n",
    "# color = 'tab:blue'\n",
    "# ax2.set_ylabel('Prediction error (MSE)', color=color)\n",
    "# ax2.plot(prediction_errors, label='Prediction error (MSE)', color=color, linestyle='--')\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# # Title and legend\n",
    "# fig.tight_layout()\n",
    "# #plt.title('Optimization and Prediction Errors over Iterations')\n",
    "# fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "\n",
    "# if live:\n",
    "#     plt.savefig(\"nonlinear_gain_errors_live.pdf\")\n",
    "# else:\n",
    "#     plt.savefig(\"nonlinear_gain_errors.pdf\")\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "ax1.pcolormesh(K_steady, vmin=-0.1, vmax=0.45, cmap='RdBu_r')\n",
    "ax1.set_title('$K_\\mathrm{steady}$')\n",
    "p2 = ax2.pcolormesh(K_opt, vmin=-0.1, vmax=0.45, cmap='RdBu_r')\n",
    "ax2.set_title('$K_\\mathrm{opt}$')\n",
    "cb_ax = fig.add_axes([.93,.124,.02,.754])\n",
    "fig.colorbar(p2,orientation='vertical',cax=cb_ax)\n",
    "\n",
    "subfolder_name = 'nonlinear_results'\n",
    "os.makedirs(subfolder_name, exist_ok=True)\n",
    "\n",
    "file_base_name = \"nonlinear_gain_matrices\"\n",
    "if H.shape[0] == 20:\n",
    "    file_base_name += \"_partial\"\n",
    "if live:\n",
    "    file_base_name += \"_live\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "plt.savefig(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax3) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "\n",
    "# Optimization Error\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Gain error ($\\|K_\\mathrm{opt} - K_\\mathrm{steady}\\|_F$)', color=color)\n",
    "line1, = ax1.plot(range(1, n_iters+1), norms[:n_iters], label='Gain error', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Instantiate a second y-axis for Prediction Error and True Divergence\n",
    "ax2 = ax1.twinx()\n",
    "color_pred = 'tab:green'\n",
    "ax2.set_ylabel('KL divergence to true filter', color=color_pred)\n",
    "#line1, = ax2.plot(prediction_errors, label='Prediction Error (MSE)', color=color_pred, linestyle='--')\n",
    "line2, = ax2.plot(range(1, n_iters+1), true_div[:n_iters], label='KL divergence to true filter', color=color_pred, linestyle='-.')\n",
    "ax2.tick_params(axis='y', labelcolor=color_pred)\n",
    "\n",
    "# Title and combined legend\n",
    "#plt.title('Gain errors and KL divergence over iterations')\n",
    "\n",
    "# Creating a combined legend for all lines\n",
    "lines = [line1, line2]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "ax3.plot(range(1, n_iters+1), prediction_errors[:n_iters])\n",
    "ax3.set_xlabel(\"Iteration\")\n",
    "ax3.set_ylabel(\"Prediction error (MSE)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "subfolder_name = 'nonlinear_results'\n",
    "os.makedirs(subfolder_name, exist_ok=True)\n",
    "\n",
    "file_base_name = \"nonlinear_gain\"\n",
    "if H.shape[0] == 20:\n",
    "    file_base_name += \"_partial\"\n",
    "if live:\n",
    "    file_base_name += \"_live\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
