{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e09d12-5017-4ada-a599-b4c6ff4e88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, lax\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "from jax.numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from jax_models import Lorenz96\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix\n",
    "#from jax_filters import ensrf_steps\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "#from jax_vi import KL_gaussian, log_likelihood\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "def create_stable_matrix(n, key):\n",
    "    # Generate a symmetric random matrix\n",
    "    A = random.normal(key, (n, n))\n",
    "    A = (A + A.T) / 2\n",
    "    \n",
    "    # Ensure the matrix has a spectral radius < 1 for stability\n",
    "    eigenvalues, eigenvectors = eigh(A)\n",
    "    scaled_eigenvalues = eigenvalues / (jnp.abs(eigenvalues).max() + 0.1)  # Scale eigenvalues to ensure stability\n",
    "    A_stable = eigenvectors @ jnp.diag(scaled_eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    return A_stable\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "n = 40\n",
    "Q = 0.1 * jnp.eye(n)  # Process noise covariance\n",
    "R_matrix = 0.5 * jnp.eye(n)#make_spd_matrix(n)  # Generating a symmetric positive definite matrix for R\n",
    "R = jnp.array(R_matrix)  # Observation noise covariance\n",
    "H = jnp.eye(n)  # Observation matrix\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "observation_interval = 1\n",
    "\n",
    "\n",
    "A_stable = create_stable_matrix(n, key)\n",
    "\n",
    "\n",
    "def state_transition_function(x):\n",
    "    return jnp.dot(A_stable, x)\n",
    "\n",
    "A_step = Partial(state_transition_function)\n",
    "\n",
    "\n",
    "num_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c41a42-cde0-4814-9be4-a04d7576725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab6d4ee-f819-4e77-97a5-2e69029c5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, A_step, observation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea281c1-54ba-4183-aee9-03d44472a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit, lax\n",
    "from jax.scipy.linalg import inv\n",
    "import numpy as np  # For handling NaN checks with JAX arrays\n",
    "\n",
    "@jit\n",
    "def kalman_filter_step(carry, input):\n",
    "    m_prev, C_prev, M, H, Q, R, observation_interval = carry\n",
    "    y_curr = input\n",
    "    m_pred = M @ m_prev\n",
    "    C_pred = M @ C_prev @ M.T + Q\n",
    "    def update():\n",
    "        S = H @ C_pred @ H.T + R\n",
    "        K = C_pred @ H.T @ inv(S)\n",
    "        y_hat = H @ m_pred\n",
    "        m_update = m_pred + K @ (y_curr - y_hat)\n",
    "        C_update = (jnp.eye(C_prev.shape[0]) - K @ H) @ C_pred\n",
    "        return m_update, C_update\n",
    "    def no_update():\n",
    "        return m_pred, C_pred\n",
    "    is_observation_available = jnp.logical_not(jnp.any(jnp.isnan(y_curr)))\n",
    "    m_update, C_update = lax.cond(is_observation_available, update, no_update)\n",
    "\n",
    "    return (m_update, C_update, M, H, Q, R, observation_interval), (m_update, C_update)\n",
    "\n",
    "def apply_kalman_filter(y, m0, C0, M, H, Q, R, observation_interval):\n",
    "    carry_init = (m0, C0, M, H, Q, R, observation_interval)\n",
    "    _, (ms, Cs) = lax.scan(kalman_filter_step, carry_init, y)\n",
    "    return ms, Cs\n",
    "\n",
    "ms, Cs = apply_kalman_filter(observations, initial_state, Q, A_stable, H, Q, R,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03646606-c688-4f79-8ea8-3b11962347c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation = 1.1\n",
    "states, covariances = ensrf_steps(A_step, n_ensemble, ensemble_init, num_steps, observations, observation_interval, H, Q, R, localization_matrix, inflation, key)\n",
    "#print(covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcb32e9c-a54b-4373-82b7-3988fefadf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jax_vi import KL_gaussian, log_likelihood, KL_sum\n",
    "\n",
    "\n",
    "# def var_cost(inflation, ensemble_init, observations, H, Q, R, localization_matrix, key, num_steps, J0):\n",
    "   \n",
    "#     states, covariances = ensrf_steps(A_step, n_ensemble, ensemble_init, num_steps, observations, observation_interval, H, Q, R, localization_matrix, inflation, key)\n",
    "\n",
    "#     ensemble_mean = jnp.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "#     key, *subkeys = random.split(key, num=N+1)\n",
    "#     kl_sum = KL_sum(ensemble_mean, covariances, n, A_step, Q, key, N)\n",
    "#     print(kl_sum)\n",
    "\n",
    "#     def inner_map(subkey):\n",
    "#         return log_likelihood(random.multivariate_normal(subkey, ensemble_mean, covariances), observations, H, R, num_steps, J0)  # Sometimes the covariances are negative definite. Fix\n",
    "#     cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "#     return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "622b4bda-86a3-428a-a910-49cb4bb6a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lax import scan\n",
    "\n",
    "def nearest_pd_matrix(M):\n",
    "    eigval, eigvec = jnp.linalg.eig(M)\n",
    "    eigval = eigval.at[eigval < 0].set(0)\n",
    "\n",
    "    return eigvec.dot(jnp.diag(eigval)).dot(eigvec.T)\n",
    "\n",
    "@jit\n",
    "def log_likelihood(v, y, H, R, J, J0):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of observations given state estimates as a sum.\n",
    "    Compares this to R, observation noise\n",
    "    \"\"\"\n",
    "    def log_likelihood_j(_, v_y):\n",
    "        v_j, y_j = v_y\n",
    "        error = y_j - H @ v_j\n",
    "        ll = error.T @ inv(R) @ error\n",
    "        return _, ll\n",
    "    _, lls = lax.scan(log_likelihood_j, None, (v, y))\n",
    "    sum_ll = jnp.nansum(lls)\n",
    "    return -0.5 * sum_ll - 0.5 * (J - J0) * jnp.log(2 * jnp.pi) - 0.5 * (J - sum(jnp.isnan(lls)) - J0) * jnp.linalg.slogdet(R)[1]\n",
    "\n",
    "\n",
    "@jit\n",
    "def KL_gaussian(m1, C1, m2, C2):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler divergence between two Gaussian distributions.\n",
    "    m1, C1: Mean and covariance of the first Gaussian distribution.\n",
    "    m2, C2: Mean and covariance of the second Gaussian distribution.\n",
    "    n: number of state variables\n",
    "    \"\"\"\n",
    "    C2_inv = inv(C2)\n",
    "    log_det_ratio = (jnp.log(jnp.linalg.eigvals(C2)).sum() - jnp.log(jnp.linalg.eigvals(C1)).sum()).real # log(det(C2) / det(C1)), works better with limited precision because the determinant is practically 0\n",
    "    return 0.5 * (log_det_ratio - n + jnp.trace(C2_inv @ C1) + ((m2 - m1).T @ C2_inv @ (m2 - m1)))\n",
    "\n",
    "@jit\n",
    "def KL_sum(m, C, Q, key):\n",
    "    \"\"\"\n",
    "    Computes the sum of KL divergences between the predicted and updated state distributions.\n",
    "    Calculates divergence between pairs of predicted and updated state distributions\n",
    "    \n",
    "    \"\"\"\n",
    "    def KL_j(_, m_C_y):\n",
    "        m_prev, m_curr, C_prev, C_curr, key = m_C_y\n",
    "        key, *subkeys_inner = random.split(key, num=N)\n",
    "        def inner_map(subkey):\n",
    "            perturbed_state = m_prev + random.multivariate_normal(subkey, jnp.zeros(n), C_prev)\n",
    "            m_pred = A_step(perturbed_state)\n",
    "            return KL_gaussian(m_curr, C_curr, m_pred, Q) \n",
    "        mean_kl = jnp.nanmean(lax.map(inner_map, jnp.array(subkeys_inner)), axis=0)\n",
    "        return _, mean_kl\n",
    "\n",
    "    _, mean_kls = scan(KL_j, None, (m[:-1, :], m[1:, :], C[:-1, :, :], C[1:, :, :], jnp.array(random.split(key, num=m.shape[0]-1))))\n",
    "    kl_sum = sum(mean_kls)\n",
    "    return kl_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b944498-0563-45de-ae15-3d73b9825baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def ledoit_wolf(P, shrinkage):\n",
    "    return (1 - shrinkage) * P + shrinkage * jnp.trace(P)/P.shape[0] * jnp.eye(P.shape[0])\n",
    "\n",
    "@jit\n",
    "def sqrtm(M):\n",
    "    eigenvalues, eigenvectors = jnp.linalg.eigh(M)\n",
    "    inv_sqrt_eigenvalues = jnp.sqrt(eigenvalues)\n",
    "    Lambda_inv_sqrt = jnp.diag(inv_sqrt_eigenvalues)\n",
    "    M_inv_sqrt = eigenvectors @ Lambda_inv_sqrt @ eigenvectors.T\n",
    "    return M_inv_sqrt.real\n",
    "\n",
    "shrinkage = 0.1\n",
    "\n",
    "@jit\n",
    "def ensrf_step(ensemble, y, H, Q, R, localization_matrix, inflation, key):\n",
    "    n_ensemble = ensemble.shape[1]\n",
    "    x_m = jnp.mean(ensemble, axis=1)\n",
    "    #ensemble += random.multivariate_normal(key, jnp.zeros(ensemble.shape[0]), Q, (n_ensemble,)).T\n",
    "    A = ensemble - x_m.reshape((-1, 1))\n",
    "    A = A*inflation\n",
    "    Pf = (A @ A.T) / (n_ensemble - 1) + Q\n",
    "    P = Pf * localization_matrix  # Element-wise multiplication for localization\n",
    "    K = P @ H.T @ jnp.linalg.inv(H @ P @ H.T + R)\n",
    "    x_m += K @ (y - H @ x_m)\n",
    "    #M = jnp.eye(x_m.shape[0]) + P @ H.T @ jnp.linalg.inv(R) @ H\n",
    "    #M_inv_sqrt = inv(jax.scipy.linalg.sqrtm(M).real)#inv_sqrtmh(M)\n",
    "    M_inv_sqrt = sqrtm(jnp.eye(x_m.shape[0]) - K@H)\n",
    "    updated_A = M_inv_sqrt @ A\n",
    "    updated_ensemble = x_m.reshape((-1, 1)) + updated_A\n",
    "    #updated_A = updated_ensemble - jnp.mean(updated_ensemble, axis=1).reshape((-1, 1))\n",
    "    updated_P = (updated_A @ updated_A.T / (n_ensemble - 1))\n",
    "    updated_P = ledoit_wolf(updated_P, shrinkage)\n",
    "    return updated_ensemble, updated_P# + jnp.eye(x_m.shape[0])*1e-4 \n",
    "\n",
    "@jit\n",
    "def ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation, key):\n",
    "    \"\"\"\n",
    "    Deterministic Ensemble Square Root Filter generalized for any model.\n",
    "    \"\"\"\n",
    "    model_vmap = jax.vmap(lambda v: state_transition_function(v), in_axes=1, out_axes=1)\n",
    "    key, *subkeys = random.split(key, num= num_steps+1)\n",
    "    subkeys = jnp.array(subkeys)\n",
    "\n",
    "    def inner(carry, t):\n",
    "        ensemble, _ = carry\n",
    "        ensemble_predicted = model_vmap(ensemble)\n",
    "\n",
    "        # Update ensemble and covariance based on observation availability\n",
    "        ensemble_updated, Pf_updated = ensrf_step(ensemble_predicted, observations[t, :], H, Q, R, localization_matrix, inflation, subkeys[t])\n",
    "    \n",
    "\n",
    "        return (ensemble_updated, Pf_updated), (ensemble_updated, Pf_updated)\n",
    "\n",
    "    # Initialize storage for covariance matrices, including one for the initial state\n",
    "    covariance_init = jnp.zeros((n,n))\n",
    "\n",
    "    # Perform the scan over timesteps with the initial ensemble and covariances\n",
    "    _, output = jax.lax.scan(inner, (ensemble_init, covariance_init), jnp.arange(num_steps))\n",
    "\n",
    "    ensembles, covariances = output\n",
    "\n",
    "    return ensembles, covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cef32e05-df4d-4e8d-ac4e-f29202f567b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def var_cost(inflation, ensemble_init, observations, H, Q, R, localization_matrix, key, J, J0):\n",
    "   \n",
    "    states, covariances = ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation, key)\n",
    "    ensemble_mean = jnp.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(ensemble_mean, covariances, Q, key)\n",
    "\n",
    "    def inner_map(subkey):\n",
    "        return log_likelihood(random.multivariate_normal(subkey, ensemble_mean, covariances), observations, H, R, J, J0)  # Sometimes the covariances are negative definite. Fix\n",
    "    cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7adff5cd-08c9-4302-adbb-c0c7c666e7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(8452.507, dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = num_steps\n",
    "var_cost(inflation + 0.8, ensemble_init, observations, H, Q, R, localization_matrix, key, J, J0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04b3580f-0d77-4ad9-adfb-a6d85b549b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d162c5d10ac4aa59e81c98e05719f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 40)\n",
      "(100, 40)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(40,), (100, 100, 100)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/util.py:286\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m cached(config\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_trace_context(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/util.py:279\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _broadcast_shapes_uncached(\u001b[38;5;241m*\u001b[39mshapes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(40,), (100, 100, 100)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(ensemble_mean\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(ms\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 46\u001b[0m true_filter_divergences\u001b[38;5;241m.\u001b[39mappend(KL_gaussian(ensemble_mean\u001b[38;5;241m.\u001b[39mT, covariances, ms\u001b[38;5;241m.\u001b[39mT, Cs))\n\u001b[1;32m     48\u001b[0m rmse \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(jnp\u001b[38;5;241m.\u001b[39mmean((ensemble_mean \u001b[38;5;241m-\u001b[39m true_states)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     49\u001b[0m rmses\u001b[38;5;241m.\u001b[39mappend(rmse)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[69], line 35\u001b[0m, in \u001b[0;36mKL_gaussian\u001b[0;34m(m1, C1, m2, C2)\u001b[0m\n\u001b[1;32m     33\u001b[0m C2_inv \u001b[38;5;241m=\u001b[39m inv(C2)\n\u001b[1;32m     34\u001b[0m log_det_ratio \u001b[38;5;241m=\u001b[39m (jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvals(C2))\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m-\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvals(C1))\u001b[38;5;241m.\u001b[39msum())\u001b[38;5;241m.\u001b[39mreal \u001b[38;5;66;03m# log(det(C2) / det(C1)), works better with limited precision because the determinant is practically 0\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (log_det_ratio \u001b[38;5;241m-\u001b[39m n \u001b[38;5;241m+\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtrace(C2_inv \u001b[38;5;241m@\u001b[39m C1) \u001b[38;5;241m+\u001b[39m ((m2 \u001b[38;5;241m-\u001b[39m m1)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m C2_inv \u001b[38;5;241m@\u001b[39m (m2 \u001b[38;5;241m-\u001b[39m m1)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:743\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 743\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maval, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:271\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    269\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m binary_op(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:98\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2, \u001b[38;5;241m/\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m promote_args(numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, x1, x2)\n\u001b[1;32m     99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/util.py:381\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    379\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    380\u001b[0m check_for_prngkeys(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m promote_shapes(fun_name, \u001b[38;5;241m*\u001b[39mpromote_dtypes(\u001b[38;5;241m*\u001b[39margs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/util.py:250\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    249\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 250\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lax\u001b[38;5;241m.\u001b[39mbroadcast_shapes(\u001b[38;5;241m*\u001b[39mshapes))\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(40,), (100, 100, 100)]"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from jax import grad\n",
    "from tqdm.notebook import tqdm\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import properscoring\n",
    "\n",
    "\n",
    "\n",
    "# Modification: Use grad to compute the gradient with respect to the inflation parameter\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "J0 = 0\n",
    "inflation_opt = 1.1  # Example starting value for inflation\n",
    "alpha = 1e-6  # Learning rate\n",
    "key = random.PRNGKey(0)  # Random key\n",
    "N = 10  # Number of MC samples\n",
    "m0 = initial_state\n",
    "C0 = Q  # Initial covariance, assuming Q is your process noise covariance\n",
    "localization_matrix = generate_gc_localization_matrix(n, 15)# jnp.ones((n, n)) # \n",
    "n_ensemble  = 20\n",
    "ensemble_init = random.multivariate_normal(key, initial_state, Q, (n_ensemble,)).T\n",
    "\n",
    "crpss = []\n",
    "rmses=[]\n",
    "inflations = []\n",
    "true_filter_divergences = []\n",
    "\n",
    "# from jax import config\n",
    "# config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "for i in tqdm(range(50)):\n",
    "    key, subkey = random.split(key)\n",
    "        \n",
    "    # Gradient descent step for inflation parameter\n",
    "    grad_inflation = var_cost_grad(inflation_opt, ensemble_init, observations, H, Q, R, localization_matrix, subkey, num_steps, J0)\n",
    "    inflation_opt -= alpha * grad_inflation  # Update inflation parameter\n",
    "    \n",
    "    inflations.append(inflation_opt)\n",
    "\n",
    "    states, covariances = ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation_opt, key)\n",
    "    \n",
    "    ensemble_mean = jnp.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "    print(ensemble_mean.shape)\n",
    "    print(ms.shape)\n",
    "    true_filter_divergences.append(KL_gaussian(ensemble_mean.T, covariances, ms.T, Cs))\n",
    "\n",
    "    rmse = jnp.sqrt(jnp.mean((ensemble_mean - true_states)**2))\n",
    "    rmses.append(rmse)\n",
    "    crps = properscoring.crps_ensemble(true_states, states).mean(axis=1).mean()\n",
    "    crpss.append(crps)\n",
    "    #clear_output(wait=True)\n",
    "    \n",
    "    print(inflation_opt, crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68bfbd18-fca0-4f90-9ffe-7d0537a51b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.07836241, -0.7641989 , -0.12098181, ..., -0.20468104,\n",
       "        -0.53091204,  0.32743043],\n",
       "       [ 0.12314163, -0.52652967, -0.26787078, ..., -0.44446802,\n",
       "        -0.34339535,  0.09057826],\n",
       "       [ 0.21506685, -0.34971774,  0.22789443, ..., -0.23412657,\n",
       "        -0.08423281,  0.46471524],\n",
       "       ...,\n",
       "       [ 0.60597956, -0.00444032, -0.76824206, ...,  0.15328997,\n",
       "        -0.3658951 , -0.06035931],\n",
       "       [ 0.06862107,  0.02553357,  0.6577666 , ...,  0.07941943,\n",
       "        -0.88178587,  0.00518188],\n",
       "       [-0.05369499,  0.08855119, -0.27075857, ..., -0.10694164,\n",
       "        -0.12803587,  0.00748018]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
