{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Generate true states and observations using the Lorenz '96 model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m observations, true_states \u001b[38;5;241m=\u001b[39m generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_model\u001b[38;5;241m.\u001b[39mstep, observation_interval)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Define the gradient of the cost function\u001b[39;00m\n\u001b[1;32m     34\u001b[0m var_cost_grad \u001b[38;5;241m=\u001b[39m grad(var_cost, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/central/home/eluk/variational_filtering/jax_models.py:93\u001b[0m, in \u001b[0;36mgenerate_true_states\u001b[0;34m(key, num_steps, n, x0, H, Q, R, model_step, observation_interval)\u001b[0m\n\u001b[1;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_steps, n))\n\u001b[1;32m     92\u001b[0m obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_steps, H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# Adjust the shape based on H\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset(x0)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_steps):\n\u001b[1;32m     96\u001b[0m     key, subkey \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'at'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, jit\n",
    "from jax.scipy.linalg import inv\n",
    "from jax.numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from jax_models import Lorenz96\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix\n",
    "from jax_filters import ensrf_steps\n",
    "from jax_vi import var_cost, KL_gaussian, log_likelihood, KL_sum\n",
    "\n",
    "\n",
    "# Parameters\n",
    "F = 8.0\n",
    "dt = 0.01\n",
    "num_steps = 300  # Number of time steps\n",
    "n = 40   # Number of state variables\n",
    "Q = 0.1 * np.eye(n)  # Process noise covariance\n",
    "R_matrix = make_spd_matrix(n)  # Generating a symmetric positive definite matrix for R\n",
    "R = np.array(R_matrix)  # Observation noise covariance\n",
    "inv_R = inv(R)\n",
    "H = np.eye(n)  # Observation matrix\n",
    "observation_interval = 1\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "\n",
    "l96_model = Lorenz96(dt = 0.01, F = 8)\n",
    "# Generate true states and observations using the Lorenz '96 model\n",
    "key = random.PRNGKey(0)\n",
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_model.step, observation_interval)\n",
    "\n",
    "\n",
    "# Define the gradient of the cost function\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "# Initial guess for K and optimization parameters\n",
    "K_opt = np.eye(n) * 0.4\n",
    "alpha = 1e-6\n",
    "\n",
    "# Optimization loop\n",
    "for i in tqdm(range(50)):\n",
    "    key, _ = random.split(key)\n",
    "    grad_K = var_cost_grad(K_opt, m, C, Q, R, H, key) # needs to be redone\n",
    "    K_opt -= alpha * grad_K\n",
    "\n",
    "    # Optional: Evaluate performance after each update\n",
    "    # e.g., recompute the estimated states and their cost\n",
    "\n",
    "print(\"Optimized K:\", K_opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
