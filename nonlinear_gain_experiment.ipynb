{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, jit\n",
    "from jax.scipy.linalg import inv\n",
    "from jax.numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from jax_models import Lorenz96\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix\n",
    "from jax_filters import ensrf_steps\n",
    "#from jax_vi import KL_gaussian, log_likelihood\n",
    "\n",
    "\n",
    "# Parameters\n",
    "F = 8.0\n",
    "dt = 0.01\n",
    "num_steps = 30  # Number of time steps\n",
    "n_timesteps = num_steps\n",
    "J = num_steps\n",
    "J0 = 0\n",
    "n = 40   # Number of state variables\n",
    "Q = 0.1 * np.eye(n)  # Process noise covariance\n",
    "R_matrix = make_spd_matrix(n)  # Generating a symmetric positive definite matrix for R\n",
    "R = np.array(R_matrix)  # Observation noise covariance\n",
    "inv_R = inv(R)\n",
    "H = np.eye(n)  # Observation matrix\n",
    "\n",
    "\n",
    "N = 10\n",
    "n_ensemble = 20\n",
    "observation_interval = 1\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "\n",
    "l96_model = Lorenz96(dt = 0.01, F = 8)\n",
    "state_transition_function = l96_model.step\n",
    "# Generate true states and observations using the Lorenz '96 model\n",
    "key = random.PRNGKey(0)\n",
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_model.step, observation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, jacfwd, lax, vmap, jacrev\n",
    "from jax.scipy.linalg import inv, det, svd\n",
    "from jax.lax import scan\n",
    "@jit\n",
    "def KL_gaussian(m1, C1, m2, C2):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler divergence between two Gaussian distributions.\n",
    "    m1, C1: Mean and covariance of the first Gaussian distribution.\n",
    "    m2, C2: Mean and covariance of the second Gaussian distribution.\n",
    "    \"\"\"\n",
    "    C2_inv = inv(C2)\n",
    "    log_det_ratio = (jnp.log(jnp.linalg.eigvals(C2)).sum() - jnp.log(jnp.linalg.eigvals(C1)).sum()).real # log(det(C2) / det(C1)), works better with limited precision because the determinant is practically 0\n",
    "    return 0.5 * (log_det_ratio - n + jnp.trace(C2_inv @ C1) + ((m2 - m1).T @ C2_inv @ (m2 - m1)))\n",
    "\n",
    "@jit\n",
    "def log_likelihood(v, y):\n",
    "    \"\"\"\n",
    "    v: State estimates.\n",
    "    y: Observations.\n",
    "    \"\"\"\n",
    "    def log_likelihood_j(_, v_y):\n",
    "        v_j, y_j = v_y\n",
    "        error = y_j - H @ v_j\n",
    "        ll = error.T @ inv_R @ error\n",
    "        return _, ll\n",
    "    _, lls = scan(log_likelihood_j, None, (v[1:, :], y))\n",
    "    sum_ll = sum(lls)\n",
    "    return -0.5 * sum_ll - 0.5 * (J - J0) * jnp.log(2 * jnp.pi) - 0.5 * (J - J0) * jnp.log(det(R))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@jit\n",
    "def KL_sum(m, C, Q, key):\n",
    "    \"\"\"\n",
    "    Computes the sum of KL divergences between the predicted and updated state distributions.\n",
    "    \"\"\"\n",
    "    def KL_j(_, m_C_y):\n",
    "        m_prev, m_curr, C_prev, C_curr, key = m_C_y\n",
    "        key, *subkeys_inner = random.split(key, num=N)\n",
    "        def inner_map(subkey):\n",
    "            perturbed_state = m_prev + random.multivariate_normal(subkey, np.zeros(n), C_prev)\n",
    "            m_pred = state_transition_function(perturbed_state)\n",
    "            return KL_gaussian(m_curr, C_curr, m_pred, Q) #not sure if use of Q here is correct\n",
    "        mean_kl = np.mean(lax.map(inner_map, np.array(subkeys_inner)), axis=0)\n",
    "        return _, mean_kl\n",
    "\n",
    "    _, mean_kls = scan(KL_j, None, (m[:-1, :], m[1:, :], C[:-1, :, :], C[1:, :, :], np.array(random.split(key, num=m.shape[0]-1))))\n",
    "    kl_sum = sum(mean_kls)\n",
    "    return kl_sum\n",
    "\n",
    "\n",
    "@jit\n",
    "def filter_step(m_C_prev, y_curr, K, Q, H, R):\n",
    "    \"\"\"\n",
    "    Apply a single forecast and update step using the Kalman filter.\n",
    "    \"\"\"\n",
    "    m_prev, C_prev = m_C_prev\n",
    "    m_pred = state_transition_function(m_prev)\n",
    "    F_jac = jacrev(state_transition_function)(m_pred)\n",
    "    m_update = (np.eye(n) - K @ H) @ m_pred + K @ y_curr\n",
    "    C_pred = F_jac @ C_prev @ F_jac.T + Q\n",
    "    C_update = (np.eye(n) - K @ H) @ C_pred @ (np.eye(n) - K @ H).T + K @ R @ K.T\n",
    "    return (m_update, C_update), (m_update, C_update)\n",
    "\n",
    "@jit\n",
    "def filtered(K, m0, C0, Q, H, R, y):\n",
    "    \"\"\"\n",
    "    Applies the filtering process to estimate the system state over time.\n",
    "    \"\"\"\n",
    "    _, m_C = scan(lambda m_C_prev, y_curr: filter_step(m_C_prev, y_curr, K, Q, H, R), (m0, C0), y)\n",
    "\n",
    "    m, C = m_C\n",
    "    return np.vstack((m0, m)), np.vstack((C0.reshape(1, n, n), C))\n",
    "\n",
    "@jit\n",
    "def var_cost(K, m0, C0, Q, H, R, y, key, J, J0):\n",
    "    \"\"\"\n",
    "    Computes the cost function for optimization, combining KL divergence and log-likelihood.\n",
    "        J, J0, H, inv_R, R, n: Parameters for log_likelihood calculation.\n",
    "    \"\"\"\n",
    "    m, C = filtered(K, m0, C0, Q, H, R,y)\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    log_likelihood_vals = lax.map(lambda subkey: log_likelihood(random.multivariate_normal(subkey, m, C), y), np.array(subkeys))\n",
    "    return (KL_sum(m, C, Q, key) - np.mean(log_likelihood_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [00:06<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24399751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "# Initial guess for K and optimization parameters\n",
    "K_opt = np.eye(n) * 0.4\n",
    "alpha = 1e-6\n",
    "key = random.PRNGKey(0)\n",
    "N = 10\n",
    "m0 = initial_state\n",
    "C0 = Q\n",
    "#kl_divergence_diff = []\n",
    "#iterations = []\n",
    "rmses = []\n",
    "norms = []\n",
    "for i in tqdm(range(20)):\n",
    "    key, _ = random.split(key)\n",
    "    m, C = filtered(K_opt, m0, C0, Q, H, R, observations)\n",
    "    rmses.append(np.sqrt(np.mean((m[1:,:] - true_states)**2)))\n",
    "    clear_output(wait=True)\n",
    "    print(np.sqrt(np.mean((m[1:,:] - true_states)**2)))\n",
    "    norms.append(np.linalg.norm(K_opt))\n",
    "    K_opt -= alpha * var_cost_grad(K_opt, initial_state, Q, Q,H, R, observations, key, num_steps, J0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
