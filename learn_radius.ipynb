{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, jit, lax\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "from jax.numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from jax_models import Lorenz96\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix\n",
    "from jax_filters import ensrf_steps\n",
    "#from jax_vi import KL_gaussian, log_likelihood\n",
    "\n",
    "\n",
    "# Parameters\n",
    "F = 8.0\n",
    "dt = 0.01\n",
    "num_steps = 30  # Number of time steps\n",
    "n_timesteps = num_steps\n",
    "J0 = 0\n",
    "n = 40   # Number of state variables\n",
    "Q = 0.1 * np.eye(n)  # Process noise covariance\n",
    "R_matrix = make_spd_matrix(n)  # Generating a symmetric positive definite matrix for R\n",
    "R = np.array(R_matrix)  # Observation noise covariance\n",
    "inv_R = inv(R)\n",
    "H = np.eye(n)  # Observation matrix\n",
    "\n",
    "\n",
    "N = 10\n",
    "n_ensemble = 20\n",
    "observation_interval = 1\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "\n",
    "l96_model = Lorenz96(dt = 0.01, F = 8)\n",
    "state_transition_function = l96_model.step\n",
    "# Generate true states and observations using the Lorenz '96 model\n",
    "key = random.PRNGKey(0)\n",
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, l96_model.step, observation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lax import scan\n",
    "\n",
    "def log_likelihood(v, y, H, inv_R, R, J, J0):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of observations given state estimates.\n",
    "    \"\"\"\n",
    "    def log_likelihood_j(_, v_y):\n",
    "        v_j, y_j = v_y\n",
    "        error = y_j - H @ v_j\n",
    "        ll = error.T @ inv_R @ error\n",
    "        return _, ll\n",
    "    _, lls = lax.scan(log_likelihood_j, None, (v, y))\n",
    "    sum_ll = sum(lls)\n",
    "    return -0.5 * sum_ll - 0.5 * (J - J0) * np.log(2 * np.pi) - 0.5 * (J - J0) * np.log(det(R))\n",
    "\n",
    "\n",
    "def KL_gaussian(m1, C1, m2, C2):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler divergence between two Gaussian distributions.\n",
    "    m1, C1: Mean and covariance of the first Gaussian distribution.\n",
    "    m2, C2: Mean and covariance of the second Gaussian distribution.\n",
    "    n: number of state variables\n",
    "    \"\"\"\n",
    "    C2_inv = inv(C2)\n",
    "    log_det_ratio = (np.log(np.linalg.eigvals(C2)).sum() - np.log(np.linalg.eigvals(C1)).sum()).real # log(det(C2) / det(C1)), works better with limited precision because the determinant is practically 0\n",
    "    return 0.5 * (log_det_ratio - n + np.trace(C2_inv @ C1) + ((m2 - m1).T @ C2_inv @ (m2 - m1)))\n",
    "\n",
    "\n",
    "def KL_sum(m, C, Q, key):\n",
    "    \"\"\"\n",
    "    Computes the sum of KL divergences between the predicted and updated state distributions.\n",
    "    \"\"\"\n",
    "    def KL_j(_, m_C_y):\n",
    "        m_prev, m_curr, C_prev, C_curr, key = m_C_y\n",
    "        key, *subkeys_inner = random.split(key, num=N)\n",
    "        def inner_map(subkey):\n",
    "            perturbed_state = m_prev + random.multivariate_normal(subkey, np.zeros(n), C_prev)\n",
    "            m_pred = state_transition_function(perturbed_state)\n",
    "            return KL_gaussian(m_curr, C_curr, m_pred, Q) #not sure if use of Q here is correct\n",
    "        mean_kl = np.mean(lax.map(inner_map, np.array(subkeys_inner)), axis=0)\n",
    "        return _, mean_kl\n",
    "\n",
    "    _, mean_kls = scan(KL_j, None, (m[:-1, :], m[1:, :], C[:-1, :, :], C[1:, :, :], np.array(random.split(key, num=m.shape[0]-1))))\n",
    "    kl_sum = sum(mean_kls)\n",
    "    return kl_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensrf_step(ensemble, y, H, Q, R, localization_matrix, inflation):\n",
    "    n_ensemble = ensemble.shape[1]\n",
    "    x_m = np.mean(ensemble, axis=1)\n",
    "    A = ensemble - x_m.reshape((-1, 1))\n",
    "    Pf = inflation * A @ A.T / (n_ensemble - 1)\n",
    "    P = Pf * localization_matrix + Q  # Element-wise multiplication for localization\n",
    "    K = P @ H.T @ np.linalg.inv(H @ P @ H.T + R)\n",
    "    x_m += K @ (y - H @ x_m)\n",
    "    M = np.eye(x_m.shape[0]) + P @ H.T @ np.linalg.inv(R) @ H\n",
    "    # U, s, Vh = svd(M)\n",
    "    # s_inv_sqrt = np.diag(s**-0.5)\n",
    "    # M_inv_sqrt = U @ s_inv_sqrt @ Vh apparently svd cannot be gradiented\n",
    "    eigenvalues, eigenvectors = eigh(M)\n",
    "    inv_sqrt_eigenvalues = 1 / np.sqrt(eigenvalues)\n",
    "    Lambda_inv_sqrt = np.diag(inv_sqrt_eigenvalues)\n",
    "    M_inv_sqrt = eigenvectors @ Lambda_inv_sqrt @ eigenvectors.T\n",
    "    updated_ensemble = x_m.reshape((-1, 1)) + M_inv_sqrt @ A\n",
    "    return updated_ensemble, P  # Now also returning P\n",
    "\n",
    "\n",
    "def ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation):\n",
    "    def inner(carry, t):\n",
    "        ensemble, covariances = carry\n",
    "        obs = observations[t, :]\n",
    "        ensemble_updated, P_updated = lax.cond(\n",
    "            t % observation_interval == 0,\n",
    "            lambda _: ensrf_step(ensemble, obs, H, Q, R, localization_matrix, inflation),\n",
    "            lambda _: (ensemble, np.zeros_like(Q)),  # Return zero covariance for non-observation steps\n",
    "            None)\n",
    "        covariances = covariances.at[t].set(P_updated)\n",
    "        return (ensemble_updated, covariances), ensemble_updated\n",
    "\n",
    "    covariances_init = np.zeros((n_timesteps, *Q.shape))\n",
    "    _, states = lax.scan(inner, (ensemble_init, covariances_init), np.arange(n_timesteps))\n",
    "\n",
    "    return states, covariances_init\n",
    "\n",
    "\n",
    "\n",
    "def var_cost(radius, ensemble_init, observations, H, Q, R, inflation, key, J, J0):\n",
    "    localization_matrix = generate_gc_localization_matrix(n, radius)\n",
    "    states, covariances = ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation)\n",
    "    ensemble_mean = np.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(ensemble_mean, covariances, Q, key)\n",
    "    \n",
    "\n",
    "    # Calculate log-likelihood values for a batch of perturbed states\n",
    "    log_likelihood_vals = lax.map(\n",
    "        lambda subkey: log_likelihood(\n",
    "            random.multivariate_normal(subkey, ensemble_mean, covariances),\n",
    "            observations, H, np.linalg.inv(R), R, J, J0),\n",
    "        np.array(subkeys))\n",
    "\n",
    "    cost = kl_sum - np.mean(log_likelihood_vals)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31645/2601952953.py:22: DeprecationWarning: Accessing jax.config via the jax.config submodule is deprecated.\n",
      "  from jax.config import config\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f337c7e97443a2b0b0f4b5e95e2cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 40, 20)\n",
      "RMSE: 0.5659351348876953\n"
     ]
    },
    {
     "ename": "FloatingPointError",
     "evalue": "invalid value (nan) encountered in jit(scan). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the de-optimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the de-optimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \n\nIt may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations. \n\nIf you see this error, consider opening a bug report at https://github.com/google/jax.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/profiler.py:336\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1163\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arrays \u001b[38;5;129;01min\u001b[39;00m out_arrays:\n\u001b[0;32m-> 1163\u001b[0m   dispatch\u001b[38;5;241m.\u001b[39mcheck_special(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, arrays)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_handler(out_arrays)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/dispatch.py:322\u001b[0m, in \u001b[0;36mcheck_special\u001b[0;34m(name, bufs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m bufs:\n\u001b[0;32m--> 322\u001b[0m   _check_special(name, buf\u001b[38;5;241m.\u001b[39mdtype, buf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/dispatch.py:327\u001b[0m, in \u001b[0;36m_check_special\u001b[0;34m(name, dtype, buf)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdebug_nans\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(np\u001b[38;5;241m.\u001b[39masarray(buf))):\n\u001b[0;32m--> 327\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFloatingPointError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid value (nan) encountered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdebug_infs\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misinf(np\u001b[38;5;241m.\u001b[39masarray(buf))):\n",
      "\u001b[0;31mFloatingPointError\u001b[0m: invalid value (nan) encountered in jit(scan)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Gradient descent step for inflation parameter\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m grad_inflation \u001b[38;5;241m=\u001b[39m var_cost_grad(radius_opt, ensemble_init, observations, H, Q, R, inflation, subkey, num_steps, J0)\n\u001b[1;32m     37\u001b[0m radius_opt \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m grad_inflation  \u001b[38;5;66;03m# Update inflation parameter\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(radius_opt)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m, in \u001b[0;36mvar_cost\u001b[0;34m(radius, ensemble_init, observations, H, Q, R, inflation, key, J, J0)\u001b[0m\n\u001b[1;32m     43\u001b[0m ensemble_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(states, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Taking the mean across the ensemble members dimension\u001b[39;00m\n\u001b[1;32m     45\u001b[0m key, \u001b[38;5;241m*\u001b[39msubkeys \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key, num\u001b[38;5;241m=\u001b[39mN\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m kl_sum \u001b[38;5;241m=\u001b[39m KL_sum(ensemble_mean, covariances, Q, key)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Calculate log-likelihood values for a batch of perturbed states\u001b[39;00m\n\u001b[1;32m     50\u001b[0m log_likelihood_vals \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m subkey: log_likelihood(\n\u001b[1;32m     52\u001b[0m         random\u001b[38;5;241m.\u001b[39mmultivariate_normal(subkey, ensemble_mean, covariances),\n\u001b[1;32m     53\u001b[0m         observations, H, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(R), R, J, J0),\n\u001b[1;32m     54\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(subkeys))\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mKL_sum\u001b[0;34m(m, C, Q, key)\u001b[0m\n\u001b[1;32m     40\u001b[0m     mean_kl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(lax\u001b[38;5;241m.\u001b[39mmap(inner_map, np\u001b[38;5;241m.\u001b[39marray(subkeys_inner)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _, mean_kl\n\u001b[0;32m---> 43\u001b[0m _, mean_kls \u001b[38;5;241m=\u001b[39m scan(KL_j, \u001b[38;5;28;01mNone\u001b[39;00m, (m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], m[\u001b[38;5;241m1\u001b[39m:, :], C[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :], C[\u001b[38;5;241m1\u001b[39m:, :, :], np\u001b[38;5;241m.\u001b[39marray(random\u001b[38;5;241m.\u001b[39msplit(key, num\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))))\n\u001b[1;32m     44\u001b[0m kl_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(mean_kls)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kl_sum\n",
      "    \u001b[0;31m[... skipping hidden 25 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/pjit.py:1209\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;66;03m# If control reaches this line, we got a NaN on the output of `compiled`\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# but not `fun.call_wrapped` on the same arguments. Let's tell the user.\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax_config.debug_nans.value and/or config.jax_debug_infs is set, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde-optimized function (i.e., the function as if the `jit` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1207\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you see this error, consider opening a bug report at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/google/jax.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFloatingPointError\u001b[39;00m(msg)\n",
      "\u001b[0;31mFloatingPointError\u001b[0m: invalid value (nan) encountered in jit(scan). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the de-optimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the de-optimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \n\nIt may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations. \n\nIf you see this error, consider opening a bug report at https://github.com/google/jax."
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from jax import grad\n",
    "from tqdm.notebook import tqdm\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "\n",
    "\n",
    "# Modification: Use grad to compute the gradient with respect to the inflation parameter\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "radius_opt = 10.01  # Example starting value for inflation\n",
    "alpha = 1e-6  # Learning rate\n",
    "key = random.PRNGKey(0)  # Random key\n",
    "N = 10  # Number of MC samples\n",
    "m0 = initial_state\n",
    "C0 = Q  # Initial covariance, assuming Q is your process noise covariance\n",
    "ensemble_init = random.multivariate_normal(key, initial_state, Q, (n_ensemble,)).T\n",
    "inflation = 1.2\n",
    "\n",
    "rmses = []\n",
    "norms = []\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "for i in tqdm(range(20)):\n",
    "    key, subkey = random.split(key)\n",
    "    localization_matrix = generate_gc_localization_matrix(n, radius_opt)\n",
    "    states, _ = ensrf_steps(ensemble_init, observations, H, Q, R, localization_matrix, inflation)\n",
    "    print(states.shape)\n",
    "    ensemble_mean = np.mean(states, axis=-1)  # Taking the mean across the ensemble members dimension\n",
    "    rmse = np.sqrt(np.mean((ensemble_mean - true_states)**2))\n",
    "    rmses.append(rmse)\n",
    "    #clear_output(wait=True)\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    \n",
    "    # Gradient descent step for inflation parameter\n",
    "    grad_inflation = var_cost_grad(radius_opt, ensemble_init, observations, H, Q, R, inflation, subkey, num_steps, J0)\n",
    "    radius_opt -= alpha * grad_inflation  # Update inflation parameter\n",
    "    print(radius_opt)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
