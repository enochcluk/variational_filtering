{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "\n",
    "from jax.lax import scan\n",
    "import jax\n",
    "from scipy.linalg import solve_discrete_are\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "\n",
    "key = random.PRNGKey(3)\n",
    "\n",
    "# System dimensions\n",
    "n = 40  # System dimension\n",
    "num_steps = 100 # number of steps\n",
    "J0 = 0 # burn in period\n",
    "N = 10 # Monte Carlo samples\n",
    "\n",
    "def create_stable_matrix(n, key):\n",
    "    # Generate a symmetric random matrix\n",
    "    A = random.normal(key, (n, n))\n",
    "    A = (A + A.T) / 2\n",
    "    \n",
    "    # Ensure the matrix has a spectral radius < 1 for stability\n",
    "    eigenvalues, eigenvectors = eigh(A)\n",
    "    scaled_eigenvalues = eigenvalues / (jnp.abs(eigenvalues).max() + 0.1)  # Scale eigenvalues to ensure stability\n",
    "    A_stable = eigenvectors @ jnp.diag(scaled_eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    return A_stable\n",
    "\n",
    "# Model parameters\n",
    "m0 = jnp.ones((n,))\n",
    "C0 = jnp.eye(n) * 1.0   # Initial state covariance matrix (P)\n",
    "q = random.normal(key, (n, n))/5\n",
    "Q = q@q.T + jnp.eye(n)*0.1#jnp.eye(n) * 5.0    # Process noise covariance matrix (Sigma in Julia code)\n",
    "R = jnp.eye(n) * 1.0    # Observation noise covariance matrix (Gamma)\n",
    "inv_R = inv(R)\n",
    "M = create_stable_matrix(n,key)    # State transition matrix (A)\n",
    "H = jnp.eye(n)          # Observation matrix\n",
    "observation_interval = 1\n",
    "\n",
    "# State initialization\n",
    "vd0 = m0 + random.multivariate_normal(key, jnp.zeros(n), C0)\n",
    "\n",
    "\n",
    "key, _ = random.split(key)\n",
    "\n",
    "\n",
    "def state_transition_function(x):\n",
    "    return jnp.dot(M, x)\n",
    "\n",
    "def jacobian_function(x):\n",
    "    return M\n",
    "\n",
    "jac_func = Partial(jacobian_function)\n",
    "A_step = Partial(state_transition_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax_vi import KL_gaussian, log_likelihood, KL_sum\n",
    "from jax_filters import apply_filtering_fixed_linear, kalman_filter_process\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observations, true_states = generate_true_states(key, num_steps, n, vd0, H, Q, R, A_step, observation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(3,10))\n",
    "def var_cost(K, m0, C0, n, M, H, Q, R, y, key, N, num_steps, J0):\n",
    "    states, covariances = apply_filtering_fixed_linear(m0, C0, y, K, n, M, H, Q, R)\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(states, covariances, n, A_step, Q, key, N)\n",
    "\n",
    "    def inner_map(subkey):\n",
    "        return log_likelihood(random.multivariate_normal(subkey, states, covariances), y, H, R, num_steps, J0)  # Sometimes the covariances are negative definite. Fix\n",
    "    cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = solve_discrete_are(M.T, H.T, Q, R)\n",
    "# Compute steady-state Kalman gain K\n",
    "K_steady = P @ H.T @ jnp.linalg.inv(H @ P @ H.T + R)\n",
    "# print(\"Steady-state K:\")\n",
    "# print(K_steady)\n",
    "\n",
    "# Define the gradient of the cost function\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "\n",
    "\n",
    "# Initial guess for K and optimization parameters\n",
    "K_opt = jnp.eye(n) * 0.1\n",
    "alpha = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m, C, K = kalman_filter_process(A_step, jac_func, m0, C0, observations, H, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ba42bd32684fb487ca23685305c492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.numpy import linalg as jnpl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "true_div = []\n",
    "prediction_errors = [] \n",
    "norms = []\n",
    "Ks = []\n",
    "\n",
    "n_iters = 500\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "    key, _ = random.split(key)\n",
    "    # Update the gradient and Kalman gain\n",
    "    grad_K = var_cost_grad(K_opt, m0, C0, n, M, H, Q, R, observations, key, N, num_steps, J0)\n",
    "    K_opt -= alpha * grad_K\n",
    "    Ks.append(K_opt)\n",
    "    # Apply filtering with the newly optimized K to generate state predictions\n",
    "    predicted_states, covariances = apply_filtering_fixed_linear(m0, C0, observations, K_opt, n, M, H, Q, R)\n",
    "    prediction_error = np.mean(np.mean((predicted_states - true_states)**2, axis=1))#jnpl.norm(predicted_states - true_states) ** 2 / len(true_states)\n",
    "    prediction_errors.append(prediction_error)\n",
    "    norms.append(jnpl.norm(K_opt - K_steady))\n",
    "    total_kl_divergence = 0\n",
    "    for t in range(num_steps):  \n",
    "        kl_div_t = KL_gaussian(n, predicted_states[t], covariances[t],  m[t], C[t])\n",
    "        total_kl_divergence += kl_div_t\n",
    "    \n",
    "    true_div.append(total_kl_divergence / num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Optimization Error\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Gain error ($\\|K_\\mathrm{opt} - K_\\mathrm{steady}\\|_F$)', color=color)\n",
    "line1, = ax1.plot(range(1, n_iters+1), norms, label='Gain error', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Instantiate a second y-axis for Prediction Error and True Divergence\n",
    "ax2 = ax1.twinx()\n",
    "color_pred = 'tab:green'\n",
    "ax2.set_ylabel('KL divergence to true filter', color=color_pred)\n",
    "#line1, = ax2.plot(prediction_errors, label='Prediction Error (MSE)', color=color_pred, linestyle='--')\n",
    "line2, = ax2.plot(range(1, n_iters+1), true_div, label='KL divergence to true filter', color=color_pred, linestyle='-.')\n",
    "ax2.tick_params(axis='y', labelcolor=color_pred)\n",
    "\n",
    "# Title and combined legend\n",
    "plt.title('Gain errors and KL divergence over iterations')\n",
    "\n",
    "# Creating a combined legend for all lines\n",
    "lines = [line1, line2]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, n_iters+1), prediction_errors)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Prediction error (MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.pcolormesh(K_steady)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.pcolormesh(K_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
