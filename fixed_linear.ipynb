{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "\n",
    "from jax.lax import scan\n",
    "import jax\n",
    "from scipy.linalg import solve_discrete_are\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "from jax.numpy import linalg as jnpl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "key = random.PRNGKey(3)\n",
    "\n",
    "# System dimensions\n",
    "n = 40  # System dimension\n",
    "num_steps = 1000 # number of steps\n",
    "J0 = 0 # burn in period\n",
    "N = 10 # Monte Carlo samples\n",
    "\n",
    "def create_stable_matrix(n, key):\n",
    "    # Generate a symmetric random matrix\n",
    "    A = random.normal(key, (n, n))\n",
    "    A = (A + A.T) / 2\n",
    "    \n",
    "    # Ensure the matrix has a spectral radius < 1 for stability\n",
    "    eigenvalues, eigenvectors = eigh(A)\n",
    "    scaled_eigenvalues = eigenvalues / (jnp.abs(eigenvalues).max() + 0.1)  # Scale eigenvalues to ensure stability\n",
    "    A_stable = eigenvectors @ jnp.diag(scaled_eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    return A_stable\n",
    "\n",
    "# Model parameters\n",
    "C0 = jnp.eye(n) * 1.0   # Initial state covariance matrix (P)\n",
    "q = random.normal(key, (n, n))/5\n",
    "Q = q@q.T + jnp.eye(n)*0.1  # Process noise covariance matrix (Sigma in Julia code)\n",
    "H = jnp.eye(n)[::2] #partial observation\n",
    "H = jnp.eye(n)\n",
    "m0 = jnp.ones((n,))\n",
    "R = jnp.eye(H.shape[0])  # R now becomes 20x20 for partial H 20*40\n",
    "inv_R = inv(R)\n",
    "M = create_stable_matrix(n,key)    # State transition matrix (A)\n",
    "observation_interval = 1\n",
    "\n",
    "# State initialization\n",
    "vd0 = m0 + random.multivariate_normal(key, jnp.zeros(n), C0)\n",
    "\n",
    "\n",
    "key, _ = random.split(key)\n",
    "\n",
    "\n",
    "def state_transition_function(x):\n",
    "    return jnp.dot(M, x)\n",
    "\n",
    "def jacobian_function(x):\n",
    "    return M\n",
    "\n",
    "jac_func = Partial(jacobian_function)\n",
    "A_step = Partial(state_transition_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax_vi import KL_gaussian, log_likelihood, KL_sum\n",
    "from jax_filters import apply_filtering_fixed_linear, kalman_filter_process, filter_step_linear\n",
    "from jax_models import visualize_observations, Lorenz96, generate_true_states, generate_gc_localization_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observations, true_states = generate_true_states(key, num_steps, n, vd0, H, Q, R, A_step, observation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(3,10))\n",
    "def var_cost(K, m0, C0, n, M, H, Q, R, y, key, N, num_steps, J0):\n",
    "    states, covariances = apply_filtering_fixed_linear(m0, C0, y, K, n, M, H, Q, R)\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(states, covariances, n, A_step, Q, key, N)\n",
    "\n",
    "    def inner_map(subkey):\n",
    "        return log_likelihood(random.multivariate_normal(subkey, states, covariances), y, H, R, num_steps, J0)  # Sometimes the covariances are negative definite. Fix\n",
    "    cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=(3))\n",
    "def var_cost_single_step(K, m0, C0, n, Q, H, R, y_curr, key, J, J0):\n",
    "    (m_update, C_update), _  = filter_step_linear((m0, C0), y_curr, K, n, M, H, Q, R)\n",
    "    \n",
    "    log_likelihood_val = log_likelihood(m_update[jnp.newaxis, :], y_curr[jnp.newaxis, :], H, R, J=1, J0=J0)\n",
    "    # Calculate the KL divergence between the predicted and updated state distributions\n",
    "    m_pred = state_transition_function(m0)\n",
    "    C_pred = M @ C0 @ M.T + Q\n",
    "    kl_divergence = KL_gaussian(n, m_update, C_update, m_pred, C_pred)\n",
    "    \n",
    "    # Combine the KL divergence and the negative log-likelihood to form the cost\n",
    "    cost = kl_divergence - log_likelihood_val\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = solve_discrete_are(M.T, H.T, Q, R)\n",
    "# Compute steady-state Kalman gain K\n",
    "K_steady = P @ H.T @ jnp.linalg.inv(H @ P @ H.T + R)\n",
    "# print(\"Steady-state K:\")\n",
    "#print(K_steady)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m, C, K = kalman_filter_process(A_step, jac_func, m0, C0, observations, H, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 40, 40)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273465cbb7724c24a27ac8af06d5260c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.numpy import linalg as jnpl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "true_div1 = []\n",
    "prediction_errors1 = [] \n",
    "norms1 = []\n",
    "Ks1 = []\n",
    "\n",
    "live = True\n",
    "\n",
    "# Define the gradient of the cost function\n",
    "var_cost_single_grad = grad(var_cost_single_step, argnums = 0)\n",
    "\n",
    "# Initial guess for K and optimization parameters\n",
    "K_opt = jnp.eye(n) * 0.1\n",
    "alpha = 1e-5\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    key, _ = random.split(key)\n",
    "    y_curr = observations[i] \n",
    "    # Update the gradient and Kalman gain\n",
    "    for j in range(100):\n",
    "        grad_K = var_cost_single_grad(K_opt, m0, C0, n, Q, H, R, y_curr, key, num_steps, J0)\n",
    "        K_opt -= alpha * grad_K\n",
    "    Ks1.append(K_opt)\n",
    "    norms1.append(jnp.linalg.norm(K_opt - K_steady)) \n",
    "    (m_update, C_update), _ = filter_step_linear((m0, C0), y_curr, K_opt, n, M, H, Q, R)\n",
    "    prediction_error = np.square(m_update - true_states[i]).mean()  # Assuming true_states[i] is available\n",
    "    prediction_errors1.append(prediction_error)\n",
    "    true_div1.append(KL_gaussian(n, m_update, C_update, m[i], C[i]))\n",
    "    # Prepare for the next step\n",
    "    m0, C0 = m_update, C_update\n",
    "\n",
    "num_iters = num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_200_errors_online = jnp.array(prediction_errors1[-200:])\n",
    "print(\"Mean error for last 200 timesteps\", jnp.mean(last_200_errors_online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if H.shape[0] == 40:\n",
    "#     K_opt = jnp.eye(n) * 0.4\n",
    "# else: #partial observation\n",
    "#     K_opt = jnp.zeros((40, 20))\n",
    "#     for i in range(0, K_opt.shape[1]):\n",
    "#         K_opt = K_opt.at[i*2, i].set(1)\n",
    "#     K_opt = K_opt + random.normal(key, K_opt.shape) * 0.3\n",
    "\n",
    "\n",
    "# true_div = []\n",
    "# prediction_errors = [] \n",
    "# norms = []\n",
    "# last_200_errors = []    # Errors for the last 200 timesteps\n",
    "# Ks = []\n",
    "# live = False\n",
    "\n",
    "\n",
    "# t = 0  \n",
    "# n_iters = 100\n",
    "# var_cost_grad = grad(var_cost, argnums=0)\n",
    "# # Initial guess for K and optimization parameters\n",
    "# key, _ = random.split(key)\n",
    "# alpha = 1e-5\n",
    "\n",
    "# for i in tqdm(range(n_iters)):\n",
    "#     key, _ = random.split(key)\n",
    "#     # Update the gradient and Kalman gain\n",
    "#     grad_K = var_cost_grad(K_opt, m0, C0, n, M, H, Q, R, observations, key, N, num_steps, J0)\n",
    "#     K_opt -= alpha * grad_K\n",
    "#     Ks.append(K_opt)\n",
    "#     # Apply filtering with the newly optimized K to generate state predictions\n",
    "#     predicted_states, covariances = apply_filtering_fixed_linear(m0, C0, observations, K_opt, n, M, H, Q, R)\n",
    "#     prediction_error = jnp.mean(jnp.mean((predicted_states - true_states)**2, axis=1))#jnpl.norm(predicted_states - true_states) ** 2 / len(true_states)\n",
    "#     prediction_errors.append(prediction_error)\n",
    "#     last_200_prediction_error = jnp.mean((predicted_states[-200:] - true_states[-200:])**2)\n",
    "#     last_200_errors.append(last_200_prediction_error)\n",
    "#     norms.append(jnpl.norm(K_opt - K_steady))\n",
    "#     total_kl_divergence = 0\n",
    "#     for t in range(num_steps):  \n",
    "#         kl_div_t = KL_gaussian(n, predicted_states[t], covariances[t],  m[t], C[t])\n",
    "#         total_kl_divergence += kl_div_t\n",
    "    \n",
    "#     true_div.append(total_kl_divergence / num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean error for last 200 timesteps (at convergence)\", jnp.mean(jnp.array(last_200_errors[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "fig, (ax1, ax3) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "# Optimization Error\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Gain error ($\\|K_\\mathrm{opt} - K_\\mathrm{steady}\\|_F$)', color=color)\n",
    "line1, = ax1.plot(range(1, n_iters+1), norms, label='Gain error', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Instantiate a second y-axis for Prediction Error and True Divergence\n",
    "ax2 = ax1.twinx()\n",
    "color_pred = 'tab:green'\n",
    "ax2.set_ylabel('KL divergence to true filter', color=color_pred)\n",
    "#line1, = ax2.plot(prediction_errors, label='Prediction Error (MSE)', color=color_pred, linestyle='--')\n",
    "line2, = ax2.plot(range(1, n_iters+1), true_div, label='KL divergence to true filter', color=color_pred, linestyle='-.')\n",
    "ax2.tick_params(axis='y', labelcolor=color_pred)\n",
    "\n",
    "# Title and combined legend\n",
    "#plt.title('Gain errors and KL divergence over iterations')\n",
    "\n",
    "# Creating a combined legend for all lines\n",
    "lines = [line1, line2]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "ax3.plot(range(1, n_iters+1), prediction_errors)\n",
    "ax3.set_xlabel(\"Iteration\")\n",
    "ax3.set_ylabel(\"Prediction error (MSE)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "subfolder_name = 'linear_results'\n",
    "os.makedirs(subfolder_name, exist_ok=True)\n",
    "\n",
    "file_base_name = \"linear_gain\"\n",
    "if H.shape[0] == 20:\n",
    "    file_base_name += \"_partial\"\n",
    "if live:\n",
    "    file_base_name += \"_live\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "plt.savefig(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "\n",
    "ax1.pcolormesh(K_steady, cmap='RdBu', vmax=0.55, vmin=-0.55)\n",
    "ax1.set_title('$K_\\mathrm{steady}$')\n",
    "p2 = ax2.pcolormesh(K_opt, cmap='RdBu', vmax=0.55, vmin=-0.55)\n",
    "ax2.set_title('$K_\\mathrm{opt}$')\n",
    "cb_ax = fig.add_axes([.93,.124,.02,.754])\n",
    "fig.colorbar(p2,orientation='vertical',cax=cb_ax)\n",
    "\n",
    "subfolder_name = 'linear_results'\n",
    "os.makedirs(subfolder_name, exist_ok=True)\n",
    "\n",
    "file_base_name = \"linear_gain_matrices\"\n",
    "if H.shape[0] == 20:\n",
    "    file_base_name += \"_partial\"\n",
    "if live:\n",
    "    file_base_name += \"_live\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "plt.savefig(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
