{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, jacfwd, jacrev\n",
    "from jax.scipy.linalg import inv, svd, eigh, det\n",
    "from jax.lax import scan\n",
    "from scipy.linalg import solve_discrete_are\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.tree_util import Partial\n",
    "from functools import partial\n",
    "from jax_vi import KL_gaussian, log_likelihood, KL_sum, plot_optimization_results\n",
    "from jax_filters import apply_filtering_fixed_nonlinear, kalman_filter_process, filter_step_nonlinear\n",
    "from jax_models import visualize_observations, Lorenz96, KuramotoSivashinsky, generate_true_states, generate_localization_matrix\n",
    "\n",
    "key = random.PRNGKey(3)\n",
    "\n",
    "# System dimensions\n",
    "n = 40  # System dimension\n",
    "p = 2  # Observation dimension\n",
    "J0 = 0 # burn in period\n",
    "N = 10 # Monte Carlo samples\n",
    "\n",
    "num_steps = 1000  # Number of time steps\n",
    "\n",
    "n = 256 # Dimensionality of the state space for KS model\n",
    "observation_interval = 5  # Interval at which observations are made\n",
    "dt = 0.25  # Time step for the KS model\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "m0 = jnp.ones((n,))\n",
    "C0 = jnp.eye(n) * 1.0   # Initial state covariance matrix (P)\n",
    "q = random.normal(key, (n, n))/5\n",
    "#Q = q@q.T + jnp.eye(n)*0.1    # Process noise covariance matrix (Sigma in Julia code) we use diagonal Q for nonlinear case\n",
    "Q = jnp.eye(n)*0.1   #jnp.eye(n) * 5.0    # Process noise covariance matrix (Sigma in Julia code)\n",
    "\n",
    "H = jnp.eye(n)          # Observation matrix\n",
    "# H = jnp.eye(n)[::2] #partial observation\n",
    "\n",
    "R = jnp.eye(H.shape[0])  # R now becomes 20x20 for partial H 20*40\n",
    "inv_R = inv(R)\n",
    "\n",
    "\n",
    "# State initialization\n",
    "initial_state = random.normal(random.PRNGKey(0), (n,))  # Initial state\n",
    "ks_model = KuramotoSivashinsky(dt=dt, s=n, l=22, M=16)\n",
    "\n",
    "state_transition_function = ks_model.step\n",
    "ks_step = Partial(state_transition_function)\n",
    "key = random.PRNGKey(0)\n",
    "jacobian_function = jacrev(ks_step, argnums=0)\n",
    "jac_func = Partial(jacobian_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observations, true_states = generate_true_states(key, num_steps, n, initial_state, H, Q, R, ks_step, observation_interval)\n",
    "y = observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(3,10))\n",
    "def var_cost(K, m0, C0, n, H, Q, R, y, key, num_steps, J0):\n",
    "    states, covariances = apply_filtering_fixed_nonlinear(m0, C0, y, K, n, ks_step, jac_func, H, Q, R)\n",
    "    key, *subkeys = random.split(key, num=N+1)\n",
    "    kl_sum = KL_sum(states, covariances, n, ks_step, Q, key, N)\n",
    "\n",
    "    def inner_map(subkey):\n",
    "        return log_likelihood(random.multivariate_normal(subkey, states, covariances), y, H, R, num_steps, J0) \n",
    "    cost = kl_sum - jnp.nanmean(jax.lax.map(inner_map, jnp.vstack(subkeys)))\n",
    "    print(cost)\n",
    "    return cost\n",
    "\n",
    "@partial(jit, static_argnums=(3))\n",
    "def var_cost_single_step(K, m0, C0, n, Q, H, R, y_curr, key, J, J0):\n",
    "    (m_update, C_update), _  =  filter_step_nonlinear((m0,C0), y_curr, K, n, ks_step, jac_func, H, Q, R)\n",
    "    \n",
    "    log_likelihood_val = log_likelihood(m_update[jnp.newaxis, :], y_curr[jnp.newaxis, :], H, R, J=1, J0=J0)\n",
    "    # Calculate the KL divergence between the predicted and updated state distributions\n",
    "    m_pred = state_transition_function(m0)\n",
    "    M = jac_func(m0)\n",
    "    C_pred = M @ C0 @ M.T + Q\n",
    "    kl_divergence = KL_gaussian(n, m_update, C_update, m_pred, C_pred)\n",
    "    \n",
    "    # Combine the KL divergence and the negative log-likelihood to form the cost\n",
    "    cost = kl_divergence - log_likelihood_val\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_m, base_C, base_K = kalman_filter_process(ks_step, jac_func, m0, C0, observations, H, Q, R)\n",
    "K_steady = jnp.mean(base_K[-10:, :, :], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.numpy import linalg as jnpl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "var_cost_grad = grad(var_cost, argnums=0)\n",
    "\n",
    "# Initial guess for K and optimization parameters\n",
    "K_opt = jnp.eye(n) * 0.4\n",
    "\n",
    "alpha = 1e-6\n",
    "\n",
    "live = False\n",
    "prediction_errors = [] \n",
    "norms = []\n",
    "true_div = []\n",
    "last_200_errors = []\n",
    "\n",
    "n_iters = 200\n",
    "num_steps = n_iters\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "    key, _ = random.split(key)\n",
    "    # Update the gradient and Kalman gain\n",
    "    grad_K = var_cost_grad(K_opt, m0, C0, n, H, Q, R, y, key, num_steps, J0)\n",
    "    K_opt -= alpha * grad_K\n",
    "    \n",
    "    # Apply filtering with the newly optimized K to generate state predictions\n",
    "    predicted_states, covariances = apply_filtering_fixed_nonlinear(m0, C0, y, K_opt, n, ks_step, jac_func, H, Q, R)\n",
    "    \n",
    "    prediction_error = np.mean(np.mean((predicted_states - true_states)**2, axis=1))\n",
    "    prediction_errors.append(prediction_error)\n",
    "    last_200_prediction_error = jnp.mean((predicted_states[-200:] - true_states[-200:])**2)\n",
    "    last_200_errors.append(last_200_prediction_error)\n",
    "    norms.append(jnpl.norm(K_opt - K_steady))\n",
    "    total_kl_divergence = 0\n",
    "    for t in range(num_steps):  \n",
    "        kl_div_t = KL_gaussian(n, predicted_states[t], covariances[t],  base_m[t], base_C[t])\n",
    "        total_kl_divergence += kl_div_t\n",
    "    \n",
    "    true_div.append(total_kl_divergence / num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jax_vi import plot_optimization_results, plot_k_matrices\n",
    "\n",
    "subfolder_name = 'ks_nonlinear_results'\n",
    "os.makedirs(subfolder_name, exist_ok=True)  # Create directory if it doesn't exist\n",
    "file_base_name = \"ks_nonlinear_gain\"\n",
    "if H.shape[0] == 20:\n",
    "    file_base_name += \"_partial\"\n",
    "if live:\n",
    "    file_base_name += \"_live\"\n",
    "file_name = file_base_name + \".pdf\"\n",
    "file_path = os.path.join(subfolder_name, file_name)\n",
    "\n",
    "plot_optimization_results(norms, prediction_errors, true_div, n_iters, ' ')\n",
    "plot_k_matrices(K_steady, K_opt, ' ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean error for last 200 timesteps (at convergence)\", jnp.mean(jnp.array(last_200_errors[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from jax.numpy import linalg as jnp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "true_div = []\n",
    "prediction_errors = [] \n",
    "norms = []\n",
    "Ks = []\n",
    "live = True\n",
    "\n",
    "# Define the gradient of the cost function\n",
    "var_cost_single_grad = grad(var_cost_single_step, argnums = 0)\n",
    "\n",
    "# Initial guess for K and optimization parameters\n",
    "alpha = 1e-5\n",
    "\n",
    "\n",
    "if H.shape[0] == 40:\n",
    "    K_opt = jnp.eye(n) * 0.4\n",
    "else: #partial obs\n",
    "    K_opt = jnp.zeros((40, 20))\n",
    "    for i in range(0, K_opt.shape[1]):\n",
    "        K_opt = K_opt.at[i*2, i].set(1)\n",
    "    #K_opt = K_opt + random.normal(key, K_opt.shape) * 0.6\n",
    "\n",
    "num_steps = 1000\n",
    "for i in tqdm(range(num_steps)):\n",
    "    key, _ = random.split(key)\n",
    "    y_curr = observations[i] \n",
    "    # Update the gradient and Kalman gain\n",
    "    for j in range(100):\n",
    "        grad_K = var_cost_single_grad(K_opt, m0, C0, n, Q, H, R, y_curr, key, num_steps, J0)\n",
    "        K_opt -= alpha * grad_K\n",
    "    Ks.append(K_opt)\n",
    "    norms.append(jnp.linalg.norm(K_opt - K_steady)) \n",
    "    (m_update, C_update), _ = filter_step_nonlinear((m0,C0), y_curr, K_opt, n, ks_step, jac_func, H, Q, R)\n",
    "    prediction_error = jnp.square(m_update - true_states[i]).mean()  # Assuming true_states[i] is available\n",
    "    prediction_errors.append(prediction_error)\n",
    "    true_div.append(KL_gaussian(n, m_update, C_update, base_m[i], base_C[i]))\n",
    "    # Prepare for the next step\n",
    "    m0, C0 = m_update, C_update\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classic_kf_prediction_errors = jnp.mean(jnp.square(base_m - true_states), axis=1)\n",
    "\n",
    "print(\"Average prediction error - Classic KF:\", jnp.mean(classic_kf_prediction_errors[-200:]),\n",
    "      \"Gradient Descent KF:\", jnp.mean(prediction_errors[-200:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
